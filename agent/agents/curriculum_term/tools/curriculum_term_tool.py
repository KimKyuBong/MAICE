"""
êµê³¼ê³¼ì • ìš©ì–´ ë¶„ì„ ë° ê²€ì¦ ë„êµ¬
ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ êµê³¼ê³¼ì •ì— ë§ëŠ” ìš©ì–´ë¥¼ ì œì•ˆí•˜ê³ , ì‘ë‹µì˜ ìš©ì–´ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import json
import logging
import os
import re
from typing import Any, Dict, List

from ...base_agent import Tool

logger = logging.getLogger(__name__)


class CurriculumTermTool(Tool):
    """êµìœ¡ê³¼ì •ê³¼ êµê³¼ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•˜ëŠ” ë„êµ¬"""
    
    def __init__(self):
        super().__init__(
            name="curriculum_term",
            description="êµìœ¡ê³¼ì •ê³¼ êµê³¼ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•˜ëŠ” ë„êµ¬"
        )
        self.curriculum_corpus = None
        # ì˜ë¯¸ í™•ì¥ì„ ìœ„í•œ ì‚¬ì „
        self._concept_relations = self._build_concept_relations()
        self._synonyms = self._build_synonyms()
        self._load_data()

    def _load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        try:
            rag_dir = os.path.join(os.path.dirname(__file__), "rag")
            db_path = os.path.join(rag_dir, "unified_corpus.db")
            
            if os.path.exists(db_path):
                self.curriculum_corpus = db_path
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.warning("ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.curriculum_corpus = None

    async def search(self, query: str, k: int = 5) -> Dict[str, Any]:
        """êµê³¼ê³¼ì •ê³¼ êµê³¼ì„œ í†µí•© ê²€ìƒ‰"""
        if not self.curriculum_corpus:
            return {"error": "ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            # êµê³¼ê³¼ì •/êµê³¼ì„œ ê²€ìƒ‰ (ì˜ë¯¸ í™•ì¥ + ì ìˆ˜í™”)
            curriculum_results = await self._search_curriculum(query, k)
            textbook_results = await self._search_textbook(query, k)
            
            return {
                "query": query,
                "curriculum_results": curriculum_results,
                "textbook_results": textbook_results,
                "total": len(curriculum_results) + len(textbook_results)
            }
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}
    
    async def _search_curriculum(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """êµê³¼ê³¼ì • ê²€ìƒ‰ (ì˜ë¯¸ í™•ì¥ + ì ìˆ˜í™”)"""
        try:
            import sqlite3
            
            conn = sqlite3.connect(self.curriculum_corpus)
            cursor = conn.cursor()
            
            query_words = self._extract_words(query)
            expanded_concepts = self._expand_query_concepts(query_words)
            direct_words = set(query_words)
            expanded_only = expanded_concepts - direct_words
            # ì•µì»¤ ê°œë…(í•µì‹¬ ê°œë…)ì´ ì¿¼ë¦¬ì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ê²°ê³¼ì—ë„ ì¡´ì¬í•˜ë„ë¡ ìš”êµ¬
            anchor_candidates = {'ë¯¸ë¶„', 'ì ë¶„', 'í•¨ìˆ˜', 'ìˆ˜ì—´', 'í™•ë¥ ', 'í†µê³„', 'ê¸°í•˜', 'ë¯¸ì ë¶„'}
            required_core = anchor_candidates.intersection(expanded_concepts)
            
            search_query = """
                SELECT id, chunk_type, title, content, keywords, 
                       grade_level, subject, topic, achievement_code
                FROM unified_chunks 
                WHERE source_type = 'curriculum' 
                AND (content LIKE ? OR title LIKE ?)
                LIMIT ?
            """
            
            all_results = []
            # í™•ì¥ëœ ê°œë…ìœ¼ë¡œ ê²€ìƒ‰ í’€ í™•ë³´
            per_term_limit = max(k, 5)
            for term in expanded_concepts:
                cursor.execute(search_query, [f'%{term}%', f'%{term}%', per_term_limit])
                results = cursor.fetchall()
                
                for result in results:
                    result_dict = {
                        "id": result[0],
                        "chunk_type": result[1],
                        "title": result[2],
                        "content": result[3],
                        "keywords": json.loads(result[4]) if result[4] else [],
                        "grade_level": result[5],
                        "subject": result[6],
                        "topic": result[7],
                        "achievement_code": result[8]
                    }
                    # ë§¤ì¹­ ë¬¸ì¥ ì¶”ì¶œ
                    match_terms = set(list(direct_words) + list(required_core))
                    if not match_terms:
                        match_terms = direct_words
                    result_dict["matched_sentences"] = self._extract_matched_sentences(
                        result_dict["content"], match_terms
                    )
                    # ê³¼ëª© í•„í„°: ìˆ˜í•™ì´ ì•„ë‹Œ ê²½ìš° ì œì™¸ (ëª…ì‹œëœ ê²½ìš°)
                    subj = result_dict.get("subject")
                    if subj and "ìˆ˜í•™" not in subj:
                        continue
                    # í•µì‹¬ ìš©ì–´ í•˜ë“œ í•„í„°: ì¿¼ë¦¬ì— í•µì‹¬ ìš©ì–´ê°€ ìˆìœ¼ë©´ ë³¸ë¬¸ì—ë„ ë°˜ë“œì‹œ í¬í•¨
                    full_text = f"{result_dict.get('title','')} {result_dict.get('content','')}"
                    if required_core and not any(core in full_text for core in required_core):
                        continue
                    # ì ìˆ˜ ê³„ì‚°
                    score = self._calculate_relevance_score(
                        item=result_dict,
                        direct_words=direct_words,
                        expanded_only=expanded_only,
                        chunk_type_weight_map={
                            "achievement_standard": 0.6,
                            "teaching_method": 0.2,
                        },
                        required_core_terms=required_core,
                        query_words=direct_words,
                    )
                    if score < 1.0:
                        continue
                    result_dict["score"] = score
                    all_results.append(result_dict)
            
            conn.close()
            
            # ì¤‘ë³µì€ ìµœê³  ì ìˆ˜ë§Œ ìœ ì§€
            id_to_best: Dict[Any, Dict[str, Any]] = {}
            for r in all_results:
                rid = r["id"]
                if rid not in id_to_best or r.get("score", 0.0) > id_to_best[rid].get("score", 0.0):
                    id_to_best[rid] = r
            ranked = sorted(id_to_best.values(), key=lambda x: x.get("score", 0.0), reverse=True)
            return ranked[:k]
            
        except Exception as e:
            logger.error(f"êµê³¼ê³¼ì • ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    async def _search_textbook(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """êµê³¼ì„œ ê²€ìƒ‰ (ì˜ë¯¸ í™•ì¥ + ì ìˆ˜í™”)"""
        try:
            import sqlite3
            
            conn = sqlite3.connect(self.curriculum_corpus)
            cursor = conn.cursor()
            
            query_words = self._extract_words(query)
            expanded_concepts = self._expand_query_concepts(query_words)
            direct_words = set(query_words)
            expanded_only = expanded_concepts - direct_words
            anchor_candidates = {'ë¯¸ë¶„', 'ì ë¶„', 'í•¨ìˆ˜', 'ìˆ˜ì—´', 'í™•ë¥ ', 'í†µê³„', 'ê¸°í•˜', 'ë¯¸ì ë¶„'}
            required_core = anchor_candidates.intersection(expanded_concepts)
            
            search_query = """
                SELECT id, chunk_type, title, content, keywords,
                       unit_number, subunit_number, grade_level, subject, topic
                FROM unified_chunks 
                WHERE source_type = 'textbook'
                AND (
                    (content LIKE ? OR title LIKE ?)
                    OR (keywords LIKE ? OR related_concepts LIKE ?)
                )
                LIMIT ?
            """
            
            all_results = []
            per_term_limit = max(k, 5)
            for term in expanded_concepts:
                cursor.execute(search_query, [f'%{term}%', f'%{term}%', f'%{term}%', f'%{term}%', per_term_limit])
                results = cursor.fetchall()
                
                for result in results:
                    result_dict = {
                        "id": result[0],
                        "chunk_type": result[1],
                        "title": result[2],
                        "content": result[3],
                        "keywords": json.loads(result[4]) if result[4] else [],
                        "unit_number": result[5],
                        "subunit_number": result[6],
                        "grade_level": result[7],
                        "subject": result[8],
                        "topic": result[9]
                    }
                    match_terms = set(list(direct_words) + list(required_core))
                    if not match_terms:
                        match_terms = direct_words
                    result_dict["matched_sentences"] = self._extract_matched_sentences(
                        result_dict["content"], match_terms
                    )
                    # ê³¼ëª© í•„í„°: ìˆ˜í•™ì´ ì•„ë‹Œ ê²½ìš° ì œì™¸ (ëª…ì‹œëœ ê²½ìš°)
                    subj = result_dict.get("subject")
                    if subj and "ìˆ˜í•™" not in subj:
                        continue
                    # í•µì‹¬ ìš©ì–´ í•˜ë“œ í•„í„° ì™„í™” (êµê³¼ì„œëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ë§¥ í—ˆìš©)
                    full_text = f"{result_dict.get('title','')} {result_dict.get('content','')}"
                    if required_core and not any(core in full_text for core in required_core):
                        keywords_str = json.dumps(result_dict.get('keywords', []), ensure_ascii=False)
                        if not any(core in keywords_str for core in required_core):
                            continue
                    score = self._calculate_relevance_score(
                        item=result_dict,
                        direct_words=direct_words,
                        expanded_only=expanded_only,
                        chunk_type_weight_map={
                            "unit_title": 0.6,
                            "concept_explanation": 0.3,
                            "example_solution": 0.2,
                        },
                        required_core_terms=required_core,
                        query_words=direct_words,
                    )
                    if score < 1.0:
                        continue
                    result_dict["score"] = score
                    all_results.append(result_dict)
            
            conn.close()
            
            id_to_best: Dict[Any, Dict[str, Any]] = {}
            for r in all_results:
                rid = r["id"]
                if rid not in id_to_best or r.get("score", 0.0) > id_to_best[rid].get("score", 0.0):
                    id_to_best[rid] = r
            ranked = sorted(id_to_best.values(), key=lambda x: x.get("score", 0.0), reverse=True)
            return ranked[:k]
            
        except Exception as e:
            logger.error(f"êµê³¼ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_words(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ"""
        words = re.findall(r'[ê°€-í£]{2,}', text)
        
        noise_words = {
            'í•œë‹¤', 'ìˆë‹¤', 'ëœë‹¤', 'ì•Šë‹¤', 'ì´ë‹¤', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'í•©ë‹ˆë‹¤',
            'ì—ì„œ', 'ìœ¼ë¡œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ê²Œ', 'ë¿ë§Œ', 'ì•„ë‹ˆë¼', 'ë˜í•œ', 'ê·¸ë¦¬ê³ ',
            'ê¸°ë³¸', 'ë‚´ìš©', 'ê´€ë ¨', 'ê°œë…', 'ì›ë¦¬', 'ë²•ì¹™'
        }
        
        meaningful_words = []
        for word in words:
            # ì¡°ì‚¬ ì œê±°ë¥¼ í†µí•œ ì •ê·œí™” (ì˜ˆ: ë¯¸ë¶„ê³¼ -> ë¯¸ë¶„, ì ë¶„ì˜ -> ì ë¶„)
            base = re.sub(r'(ê³¼|ì™€|ì˜|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ)$', '', word)
            candidate = base if len(base) >= 2 else word
            if candidate not in noise_words and candidate not in meaningful_words:
                meaningful_words.append(candidate)
        
        return meaningful_words[:10]

    def _extract_matched_sentences(self, text: str, terms: set, window: int = 1, max_sentences: int = 6) -> List[str]:
        """ì¿¼ë¦¬/í•µì‹¬ ìš©ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ê³¼ ì£¼ë³€ ë¬¸ì¥ì„ ì¶”ì¶œ"""
        if not text:
            return []
        # ë¬¸ì¥ ë¶„í•  (ê°„ë‹¨í•œ í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„)
        sentences = re.split(r'(?<=[\.!?ã€‚ï¼ï¼Ÿ])\s+|\n+', text)
        sentences = [s.strip() for s in sentences if s and s.strip()]
        matched_indices = []
        for idx, sent in enumerate(sentences):
            if any(term and term in sent for term in terms):
                matched_indices.append(idx)
        # ì£¼ë³€ ë¬¸ì¥ í¬í•¨
        selected = []
        seen = set()
        for idx in matched_indices:
            start = max(0, idx - window)
            end = min(len(sentences), idx + window + 1)
            for j in range(start, end):
                if j not in seen:
                    seen.add(j)
                    selected.append(sentences[j])
                if len(selected) >= max_sentences:
                    break
            if len(selected) >= max_sentences:
                break
        return selected

    def _build_concept_relations(self) -> Dict[str, set]:
        """í•µì‹¬ ê°œë… ê°„ ì—°ê´€ ê´€ê³„ ì‚¬ì „"""
        return {
            'ë¯¸ë¶„': {'ë¯¸ë¶„ë²•', 'ë¯¸ë¶„ê³„ìˆ˜', 'ë„í•¨ìˆ˜', 'ì ‘ì„ ', 'ë³€í™”ìœ¨', 'ì—°ì†', 'ê·¹í•œ', 'ì ë¶„', 'ë¯¸ì ë¶„'},
            'ì ë¶„': {'ì •ì ë¶„', 'ë¶€ì •ì ë¶„', 'ë©´ì ', 'ëˆ„ì ', 'ì ë¶„ë²•', 'ë¯¸ì ë¶„'},
            'í•¨ìˆ˜': {'ì •ì˜ì—­', 'ì¹˜ì—­', 'ê³µì—­', 'ê·¸ë˜í”„', 'í•©ì„±í•¨ìˆ˜', 'ì—­í•¨ìˆ˜'},
            'ìˆ˜ì—´': {'ë“±ì°¨ìˆ˜ì—´', 'ë“±ë¹„ìˆ˜ì—´', 'ì¼ë°˜í•­', 'ë¶€ë¶„í•©', 'ê³µì°¨', 'ê³µë¹„'},
            'í™•ë¥ ': {'í†µê³„', 'í™•ë¥ ë¶„í¬', 'ê¸°ëŒ“ê°’', 'ë¶„ì‚°'},
        }

    def _build_synonyms(self) -> Dict[str, set]:
        """ë™ì˜ì–´/ìœ ì‚¬ì–´ ì‚¬ì „"""
        return {
            'ë¯¸ë¶„': {'ë„í•¨ìˆ˜', 'ë³€í™”ìœ¨'},
            'ë„í•¨ìˆ˜': {'ë¯¸ë¶„'},
            'ì ë¶„': {'ëˆ„ì ', 'ë©´ì '},
            'í•¨ìˆ˜': {'ëŒ€ì‘'},
            'ìˆ˜ì—´': {'ì—´'},
        }

    def _expand_query_concepts(self, query_words: List[str]) -> set:
        """ì¿¼ë¦¬ ê°œë… í™•ì¥: ì—°ê´€ ê°œë… + ë™ì˜ì–´ í¬í•¨"""
        expanded = set(query_words)
        for w in query_words:
            if w in self._concept_relations:
                expanded.update(self._concept_relations[w])
            if w in self._synonyms:
                expanded.update(self._synonyms[w])
        return expanded

    def _calculate_relevance_score(
        self,
        item: Dict[str, Any],
        direct_words: set,
        expanded_only: set,
        chunk_type_weight_map: Dict[str, float],
        required_core_terms: set,
        query_words: set,
    ) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        text = f"{item.get('title', '')} {item.get('content', '')}"
        # í•œêµ­ì–´ëŠ” lower ì˜í–¥ ê±°ì˜ ì—†ìŒ
        direct_hits = sum(1 for w in direct_words if w and w in text)
        related_hits = sum(1 for w in expanded_only if w and w in text)
        score = direct_hits * 2.0 + related_hits * 1.0
        # chunk íƒ€ì… ê°€ì¤‘ì¹˜
        weight = chunk_type_weight_map.get(item.get('chunk_type'), 0.0)
        score += weight
        # ë‚´ìš© ê¸¸ì´ë¡œ ì•½í•œ ë³´ì • (ì§§ì€ ë…¸ì´ì¦ˆ ë°©ì§€)
        content_len = len(item.get('content') or '')
        if content_len >= 200:
            score += 0.2
        elif content_len >= 80:
            score += 0.1
        # ì¿¼ë¦¬ì— ì•µì»¤ ê°œë…ì´ í¬í•¨ë˜ì—ˆëŠ”ë° ë³¸ë¬¸ì— í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê°•í•œ íŒ¨ë„í‹°
        if required_core_terms:
            if not any(core in text for core in required_core_terms):
                score -= 2.0
        # 'ê¸°ë³¸ ì •ë¦¬' ë¬¸ë§¥ ë³´ì •: ë¯¸ë¶„/ì ë¶„ê³¼ í•¨ê»˜ ë“±ì¥ ì‹œ ê°€ì‚°ì 
        if ('ê¸°ë³¸' in query_words or 'ì •ë¦¬' in query_words) and any(core in required_core_terms for core in {'ë¯¸ë¶„', 'ì ë¶„'}):
            if ('ë¯¸ë¶„' in text and 'ì ë¶„' in text and 'ì •ë¦¬' in text) or ('ê¸°ë³¸' in text and 'ì •ë¦¬' in text and any(core in text for core in {'ë¯¸ë¶„', 'ì ë¶„'})):
                score += 0.5
        return score

    async def execute(self, action: str, **kwargs) -> Dict[str, Any]:
        """ë„êµ¬ ì‹¤í–‰"""
        try:
            if action == "search":
                query = kwargs.get("query", "")
                k = kwargs.get("k", 5)
                return await self.search(query, k)
            else:
                return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” ì•¡ì…˜: {action}"}
                
        except Exception as e:
            logger.error(f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    async def analyze_question(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ êµê³¼ê³¼ì •ì— ë§ëŠ” ìš©ì–´ë¥¼ ì œì•ˆ"""
        try:
            # ì§ˆë¬¸ì—ì„œ í•µì‹¬ ê°œë… ì¶”ì¶œ
            core_concepts = self._extract_core_concepts(question)
            
            # ê° ê°œë…ì— ëŒ€í•´ êµê³¼ê³¼ì • ê²€ìƒ‰
            suggested_terms = []
            concept_level = "ê³ ë“±í•™êµ 1í•™ë…„"  # ê¸°ë³¸ê°’
            avoid_terms = []
            achievement_standards = []
            teaching_notes = []
            textbook_examples = []
            
            for concept in core_concepts:
                search_result = await self.search(concept, k=5)
                
                if "error" not in search_result:
                    # êµê³¼ê³¼ì • ê²°ê³¼ì—ì„œ ì ì ˆí•œ ìš©ì–´ì™€ ì„±ì·¨ê¸°ì¤€ ì¶”ì¶œ
                    curriculum_data = self._extract_curriculum_data(search_result.get("curriculum_results", []))
                    suggested_terms.extend(curriculum_data.get("terms", []))
                    achievement_standards.extend(curriculum_data.get("achievement_standards", []))
                    teaching_notes.extend(curriculum_data.get("teaching_notes", []))
                    
                    # êµê³¼ì„œ ê²°ê³¼ì—ì„œ ê°œë… ìˆ˜ì¤€ê³¼ ì˜ˆì‹œ ì¶”ì¶œ
                    textbook_data = self._extract_textbook_data(search_result.get("textbook_results", []))
                    if textbook_data.get("level"):
                        concept_level = textbook_data["level"]
                    textbook_examples.extend(textbook_data.get("examples", []))
                
                # ê³ ê¸‰/ì „ë¬¸ ìš©ì–´ ì‹ë³„ (ì‹¤ì œ êµê³¼ê³¼ì • ë°ì´í„° ê¸°ë°˜)
                advanced_terms = await self._identify_advanced_terms_dynamically(concept)
                if advanced_terms:
                    avoid_terms.extend(advanced_terms)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            suggested_terms = list(set(suggested_terms))[:15]
            avoid_terms = list(set(avoid_terms))[:8]
            achievement_standards = list(set(achievement_standards))[:5]
            teaching_notes = list(set(teaching_notes))[:5]
            textbook_examples = list(set(textbook_examples))[:5]
            
            return {
                "success": True,
                "suggested_terms": suggested_terms,
                "concept_level": concept_level,
                "avoid_terms": avoid_terms,
                "achievement_standards": achievement_standards,
                "teaching_notes": teaching_notes,
                "textbook_examples": textbook_examples,
                "analysis": f"ì§ˆë¬¸ì—ì„œ {len(core_concepts)}ê°œ í•µì‹¬ ê°œë…ì„ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.",
                "recommendations": self._generate_recommendations_with_context(
                    suggested_terms, concept_level, achievement_standards, teaching_notes, textbook_examples
                )
            }
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def verify_terms(self, content: str) -> Dict[str, Any]:
        """ì‘ë‹µ ë‚´ìš©ì˜ ìš©ì–´ë¥¼ ê²€ì¦í•˜ê³  êµê³¼ê³¼ì •ì— ë§ê²Œ ìˆ˜ì • ì œì•ˆ"""
        try:
            # ë‚´ìš©ì—ì„œ ìˆ˜í•™ ìš©ì–´ ì¶”ì¶œ
            math_terms = self._extract_math_terms(content)
            
            violations = []
            suggestions = []
            corrected_text = content
            
            for term in math_terms:
                # ìš©ì–´ê°€ êµê³¼ê³¼ì •ì— ì í•©í•œì§€ ê²€ì¦
                verification_result = await self._verify_single_term(term)
                
                if not verification_result["is_appropriate"]:
                    violations.append({
                        "term": term,
                        "issue": verification_result["issue"],
                        "suggestion": verification_result["suggestion"]
                    })
                    
                    # ìˆ˜ì • ì œì•ˆ
                    if verification_result["suggestion"]:
                        corrected_text = corrected_text.replace(term, verification_result["suggestion"])
                        suggestions.append({
                            "original": term,
                            "replacement": verification_result["suggestion"],
                            "reason": verification_result["issue"]
                        })
            
            return {
                "success": True,
                "violations": violations,
                "suggestions": suggestions,
                "corrected_text": corrected_text,
                "total_terms_checked": len(math_terms)
            }
            
        except Exception as e:
            logger.error(f"ìš©ì–´ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _extract_core_concepts(self, question: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ ìˆ˜í•™ ê°œë… ì¶”ì¶œ"""
        # ìˆ˜í•™ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
        math_patterns = [
            r'ë¯¸ë¶„|ì ë¶„|í•¨ìˆ˜|ìˆ˜ì—´|í™•ë¥ |í†µê³„|ê¸°í•˜|ì‚¼ê°í•¨ìˆ˜|ì§€ìˆ˜í•¨ìˆ˜|ë¡œê·¸í•¨ìˆ˜',
            r'ë°©ì •ì‹|ë¶€ë“±ì‹|ì§‘í•©|ëª…ì œ|ë²¡í„°|í–‰ë ¬',
            r'ê·¹í•œ|ì—°ì†|ë„í•¨ìˆ˜|ì •ì ë¶„|ë¶€ì •ì ë¶„',
            r'ë“±ì°¨ìˆ˜ì—´|ë“±ë¹„ìˆ˜ì—´|ìˆ˜í•™ì  ê·€ë‚©ë²•',
            r'ì‚¬ì¸ë²•ì¹™|ì½”ì‚¬ì¸ë²•ì¹™|ì‚¼ê°í˜•|ì›|í¬ë¬¼ì„ |íƒ€ì›|ìŒê³¡ì„ '
        ]
        
        concepts = []
        for pattern in math_patterns:
            matches = re.findall(pattern, question)
            concepts.extend(matches)
        
        return list(set(concepts))
    
    def _extract_curriculum_data(self, curriculum_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """êµê³¼ê³¼ì • ê²°ê³¼ì—ì„œ ì ì ˆí•œ ìš©ì–´ì™€ ì„±ì·¨ê¸°ì¤€, ìœ ì˜ì‚¬í•­, ì˜ˆì‹œ ì¶”ì¶œ"""
        terms = []
        achievement_standards = []
        teaching_notes = []
        textbook_examples = []
        
        for result in curriculum_results:
            # ê²°ê³¼ê°€ íŠœí”Œì¸ ê²½ìš° (ì§ì ‘ SQL ê²°ê³¼)
            if isinstance(result, tuple):
                if len(result) >= 4:
                    title = result[2] or ""
                    content = result[3] or ""
                    
                    # ì„±ì·¨ê¸°ì¤€ì—ì„œ í•µì‹¬ ìš©ì–´ ì¶”ì¶œ
                    if "ì„±ì·¨ê¸°ì¤€" in title or "ì„±ì·¨ê¸°ì¤€" in content:
                        # ì½œë¡  ë’¤ì˜ í•µì‹¬ ë‚´ìš©ì—ì„œ ìš©ì–´ ì¶”ì¶œ
                        if ":" in content:
                            core_content = content.split(":")[-1]
                            # ì‰¼í‘œë‚˜ ë§ˆì¹¨í‘œë¡œ êµ¬ë¶„ëœ ìš©ì–´ë“¤
                            content_terms = re.findall(r'[ê°€-í£]+', core_content)
                            terms.extend(content_terms[:3])  # ìƒìœ„ 3ê°œë§Œ
                        
                        # ì„±ì·¨ê¸°ì¤€ì„ ê°„ê²°í•˜ê²Œ ì •ë¦¬
                        clean_content = self._clean_achievement_standard(content)
                        if clean_content:
                            achievement_standards.append(clean_content)
                    
                    # í‚¤ì›Œë“œì—ì„œ ìš©ì–´ ì¶”ì¶œ
                    if len(result) > 4 and result[4]:
                        try:
                            keywords = json.loads(result[4]) if isinstance(result[4], str) else result[4]
                            if isinstance(keywords, list):
                                terms.extend(keywords[:5])
                        except:
                            pass
                    
                    # ìœ ì˜ì‚¬í•­ ì¶”ì¶œ (teaching_methodì—ì„œ)
                    if "ìœ ì˜ì‚¬í•­" in content or "ì£¼ì˜" in content or "ê²½ê³ " in content:
                        clean_note = self._clean_teaching_note(content)
                        if clean_note:
                            teaching_notes.append(clean_note)
                    
                    # ì˜ˆì‹œ ì¶”ì¶œ
                    if "ì˜ˆì‹œ" in content or "ì˜ˆì œ" in content or "ë¬¸ì œ" in content:
                        clean_example = self._clean_example(content)
                        if clean_example:
                            textbook_examples.append(clean_example)
            
            # ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (ì²˜ë¦¬ëœ ê²°ê³¼)
            elif isinstance(result, dict):
                title = result.get("title", "")
                content = result.get("content", "")
                keywords = result.get("keywords", [])
                
                # ì„±ì·¨ê¸°ì¤€ì—ì„œ í•µì‹¬ ìš©ì–´ ì¶”ì¶œ
                if "ì„±ì·¨ê¸°ì¤€" in title or "ì„±ì·¨ê¸°ì¤€" in content:
                    if ":" in content:
                        core_content = content.split(":")[-1]
                        content_terms = re.findall(r'[ê°€-í£]+', core_content)
                        terms.extend(content_terms[:3])
                    
                    # ì„±ì·¨ê¸°ì¤€ì„ ê°„ê²°í•˜ê²Œ ì •ë¦¬
                    clean_content = self._clean_achievement_standard(content)
                    if clean_content:
                        achievement_standards.append(clean_content)
                
                # í‚¤ì›Œë“œì—ì„œ ìš©ì–´ ì¶”ì¶œ
                if keywords:
                    if isinstance(keywords, list):
                        terms.extend(keywords[:5])
                    elif isinstance(keywords, str):
                        try:
                            parsed_keywords = json.loads(keywords)
                            if isinstance(parsed_keywords, list):
                                terms.extend(parsed_keywords[:5])
                        except:
                            pass
                
                # ìœ ì˜ì‚¬í•­ ì¶”ì¶œ
                if "ìœ ì˜ì‚¬í•­" in content or "ì£¼ì˜" in content or "ê²½ê³ " in content:
                    clean_note = self._clean_teaching_note(content)
                    if clean_note:
                        teaching_notes.append(clean_note)
                
                # ì˜ˆì‹œ ì¶”ì¶œ
                if "ì˜ˆì‹œ" in content or "ì˜ˆì œ" in content or "ë¬¸ì œ" in content:
                    clean_example = self._clean_example(content)
                    if clean_example:
                        textbook_examples.append(clean_example)
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        filtered_terms = []
        for term in terms:
            if len(term) >= 2 and term not in filtered_terms:
                filtered_terms.append(term)
            
            return {
            "terms": filtered_terms[:10],
            "achievement_standards": achievement_standards[:3],
            "teaching_notes": teaching_notes[:3],
            "textbook_examples": textbook_examples[:3]
        }
    
    def _clean_achievement_standard(self, content: str) -> str:
        """ì„±ì·¨ê¸°ì¤€ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì •ë¦¬"""
        if not content:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì œê±°
        content = re.sub(r'\s+', ' ', content.strip())
        
        # ì„±ì·¨ê¸°ì¤€ ì½”ë“œ íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: [2ìˆ˜01-01], [3ìˆ˜01-01] ë“±)
        achievement_code = ""
        code_pattern = r'\[([ê°€-í£]+\d+-\d+)\]'
        code_match = re.search(code_pattern, content)
        if code_match:
            achievement_code = code_match.group(1)
            # ì½”ë“œ ì´í›„ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
            code_end = content.find(']') + 1
            content = content[code_end:].strip()
        
        # ì„±ì·¨ê¸°ì¤€ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ì•ì— ë¶™ì´ê¸°
        if achievement_code:
            prefix = f"ì„±ì·¨ê¸°ì¤€ [{achievement_code}]: "
        else:
            # ì½”ë“œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì„±ì·¨ê¸°ì¤€ í˜•ì‹ ì‚¬ìš©
            prefix = "ì„±ì·¨ê¸°ì¤€: "
        
        # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ í•µì‹¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if len(content) > 100:
            # í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', content)
            for sentence in sentences:
                if any(keyword in sentence for keyword in ['í•¨ìˆ˜', 'ìˆ˜ì—´', 'ë¯¸ë¶„', 'ì ë¶„', 'í™•ë¥ ', 'í†µê³„', 'ê¸°í•˜', 'ë°©ì •ì‹', 'ë„í˜•', 'ì¸¡ì •']):
                    if len(sentence.strip()) > 20:
                        return prefix + sentence.strip()[:80] + "..."
            
            # ì²« ë²ˆì§¸ ë¬¸ì¥ ì‚¬ìš©
            return prefix + content[:80] + "..."
        
        return prefix + content
    
    def _clean_teaching_note(self, content: str) -> str:
        """ìœ ì˜ì‚¬í•­ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì •ë¦¬"""
        if not content:
            return ""
        
        content = re.sub(r'\s+', ' ', content.strip())
        
        # í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ
        if len(content) > 100:
            # ìœ ì˜ì‚¬í•­ í‚¤ì›Œë“œ ì£¼ë³€ ë‚´ìš©
            for keyword in ['ìœ ì˜ì‚¬í•­', 'ì£¼ì˜', 'ê²½ê³ ']:
                if keyword in content:
                    start = content.find(keyword)
                    end = min(start + 100, len(content))
                    return content[start:end] + "..."
            
            return content[:100] + "..."
        
        return content
    
    def _clean_example(self, content: str) -> str:
        """ì˜ˆì‹œ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì •ë¦¬"""
        if not content:
            return ""
        
        content = re.sub(r'\s+', ' ', content.strip())
        
        # ì˜ˆì‹œ í‚¤ì›Œë“œ ì£¼ë³€ ë‚´ìš©ë§Œ ì¶”ì¶œ
        for keyword in ['ì˜ˆì‹œ', 'ì˜ˆì œ', 'ë¬¸ì œ']:
            if keyword in content:
                start = content.find(keyword)
                end = min(start + 80, len(content))
                return content[start:end] + "..."
        
        # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ìë¥´ê¸°
        if len(content) > 80:
            return content[:80] + "..."
        
        return content
    
    def _extract_textbook_data(self, textbook_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """êµê³¼ì„œ ê²°ê³¼ì—ì„œ ê°œë… ìˆ˜ì¤€ê³¼ ì˜ˆì‹œ ì¶”ì¶œ"""
        level = "ê³ ë“±í•™êµ 1í•™ë…„"
        examples = []
        
        for result in textbook_results:
            if isinstance(result, tuple) and len(result) > 1:
                chunk_type = result[1]
                content = result[3] if len(result) > 3 else ""
                
                # chunk_typeì— ë”°ë¥¸ ìˆ˜ì¤€ íŒë‹¨
                if chunk_type == "concept_explanation":
                    level = "ê³ ë“±í•™êµ 2-3í•™ë…„"  # ê°œë… ì„¤ëª…ì€ ê³ ê¸‰ ìˆ˜ì¤€
                elif chunk_type == "unit_title":
                    # ë‹¨ì› ì œëª©ì—ì„œ í•™ë…„ ì •ë³´ ì¶”ì¶œ
                    if content:
                        if "ë¯¸ì ë¶„" in content or "ê¸°í•˜" in content:
                            level = "ê³ ë“±í•™êµ 3í•™ë…„"
                        elif "í•¨ìˆ˜" in content or "ìˆ˜ì—´" in content or "í™•ë¥ " in content:
                            level = "ê³ ë“±í•™êµ 2í•™ë…„"
                        elif "ë°©ì •ì‹" in content or "ë„í˜•" in content:
                            level = "ê³ ë“±í•™êµ 1í•™ë…„"
                
                # êµê³¼ì„œ ì˜ˆì‹œ ì¶”ì¶œ (ê°„ê²°í•˜ê²Œ)
                if content and len(content) > 20:  # ë„ˆë¬´ ì§§ì€ ë‚´ìš©ì€ ì œì™¸
                    clean_example = self._clean_textbook_example(content, chunk_type)
                    if clean_example:
                        examples.append(clean_example)
            
            elif isinstance(result, dict):
                chunk_type = result.get("chunk_type", "")
                content = result.get("content", "")
                
                # chunk_typeì— ë”°ë¥¸ ìˆ˜ì¤€ íŒë‹¨
                if chunk_type == "concept_explanation":
                    level = "ê³ ë“±í•™êµ 2-3í•™ë…„"
                elif chunk_type == "unit_title":
                    if content:
                        if "ë¯¸ì ë¶„" in content or "ê¸°í•˜" in content:
                            level = "ê³ ë“±í•™êµ 3í•™ë…„"
                        elif "í•¨ìˆ˜" in content or "ìˆ˜ì—´" in content or "í™•ë¥ " in content:
                            level = "ê³ ë“±í•™êµ 2í•™ë…„"
                        elif "ë°©ì •ì‹" in content or "ë„í˜•" in content:
                            level = "ê³ ë“±í•™êµ 1í•™ë…„"
                
                # êµê³¼ì„œ ì˜ˆì‹œ ì¶”ì¶œ (ê°„ê²°í•˜ê²Œ)
                if content and len(content) > 20:
                    clean_example = self._clean_textbook_example(content, chunk_type)
                    if clean_example:
                        examples.append(clean_example)
            
            return {
            "level": level,
            "examples": examples[:3]  # ìµœëŒ€ 3ê°œ ì˜ˆì‹œ
        }
    
    def _clean_textbook_example(self, content: str, chunk_type: str) -> str:
        """êµê³¼ì„œ ì˜ˆì‹œ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì •ë¦¬"""
        if not content:
            return ""
        
        content = re.sub(r'\s+', ' ', content.strip())
        
        # chunk_typeì— ë”°ë¥¸ ì²˜ë¦¬
        if chunk_type == "example_solution":
            # ë¬¸ì œ í’€ì´ ì˜ˆì‹œëŠ” í•µì‹¬ë§Œ ì¶”ì¶œ
            if len(content) > 100:
                # ë¬¸ì œì™€ ë‹µì•ˆ ë¶€ë¶„ ì°¾ê¸°
                for keyword in ['ë¬¸ì œ', 'í’€ì´', 'ë‹µì•ˆ', 'í•´ë‹µ']:
                    if keyword in content:
                        start = content.find(keyword)
                        end = min(start + 100, len(content))
                        return content[start:end] + "..."
                return content[:100] + "..."
        
        elif chunk_type == "concept_explanation":
            # ê°œë… ì„¤ëª…ì€ í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ
            if len(content) > 120:
                sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', content)
                for sentence in sentences:
                    if any(keyword in sentence for keyword in ['í•¨ìˆ˜', 'ìˆ˜ì—´', 'ë¯¸ë¶„', 'ì ë¶„', 'í™•ë¥ ', 'í†µê³„', 'ê¸°í•˜', 'ë°©ì •ì‹']):
                        if len(sentence.strip()) > 30:
                            return sentence.strip()[:120] + "..."
                return content[:120] + "..."
        
        # ì¼ë°˜ì ì¸ ê²½ìš°
        if len(content) > 80:
            return content[:80] + "..."
        
        return content
    
    async def _identify_advanced_terms_dynamically(self, concept: str) -> List[str]:
        """ê³ ê¸‰/ì „ë¬¸ ìš©ì–´ ì‹ë³„ (ì‹¤ì œ êµê³¼ê³¼ì • ë°ì´í„° ê¸°ë°˜)"""
        advanced_terms = []
        
        try:
            import sqlite3
            
            conn = sqlite3.connect(self.curriculum_corpus)
            cursor = conn.cursor()
            
            # 1. êµê³¼ê³¼ì •ì—ì„œ í•´ë‹¹ ê°œë…ì˜ ìˆ˜ì¤€ í™•ì¸
            level_query = """
                SELECT chunk_type, content, keywords
                FROM unified_chunks
                WHERE source_type = 'curriculum'
                AND (content LIKE ? OR title LIKE ? OR keywords LIKE ?)
                LIMIT 10
            """
            cursor.execute(level_query, [f'%{concept}%', f'%{concept}%', f'%{concept}%'])
            curriculum_results = cursor.fetchall()
            
            # 2. êµê³¼ì„œì—ì„œ í•´ë‹¹ ê°œë…ì˜ ìˆ˜ì¤€ í™•ì¸
            textbook_query = """
                SELECT chunk_type, content, keywords, unit_number
                FROM unified_chunks
                WHERE source_type = 'textbook'
                AND (content LIKE ? OR title LIKE ? OR keywords LIKE ?)
                LIMIT 10
            """
            cursor.execute(textbook_query, [f'%{concept}%', f'%{concept}%', f'%{concept}%'])
            textbook_results = cursor.fetchall()
            
            # 3. ê³ ê¸‰ ê°œë… íŒë‹¨ ê¸°ì¤€
            advanced_indicators = {
                'chunk_type': ['concept_explanation', 'advanced_concept'],
                'content_keywords': ['ê³ ê¸‰', 'ì‹¬í™”', 'ì „ë¬¸', 'ëŒ€í•™', 'ì—°êµ¬', 'ì´ë¡ '],
                'unit_patterns': ['3í•™ë…„', 'ê³ ê¸‰', 'ì‹¬í™”', 'ì„ íƒ']
            }
            
            # êµê³¼ê³¼ì • ê²°ê³¼ ë¶„ì„
            for result in curriculum_results:
                chunk_type = result[0] if len(result) > 0 else ""
                content = result[1] if len(result) > 1 else ""
                keywords = result[2] if len(result) > 2 else ""
                
                # ê³ ê¸‰ ì§€í‘œ í™•ì¸
                if any(indicator in content for indicator in advanced_indicators['content_keywords']):
                    advanced_terms.append(concept)
                    break
                
                # í‚¤ì›Œë“œì—ì„œ ê³ ê¸‰ ì§€í‘œ í™•ì¸
                if keywords:
                    try:
                        parsed_keywords = json.loads(keywords) if isinstance(keywords, str) else keywords
                        if isinstance(parsed_keywords, list):
                            for keyword in parsed_keywords:
                                if any(indicator in str(keyword) for indicator in advanced_indicators['content_keywords']):
                                    advanced_terms.append(concept)
                                    break
                    except:
                        pass
            
            # êµê³¼ì„œ ê²°ê³¼ ë¶„ì„
            for result in textbook_results:
                chunk_type = result[0] if len(result) > 0 else ""
                content = result[1] if len(result) > 1 else ""
                unit_number = result[3] if len(result) > 3 else ""
                
                # chunk_typeì´ concept_explanationì´ë©´ ê³ ê¸‰ ê°œë… ê°€ëŠ¥ì„±
                if chunk_type in advanced_indicators['chunk_type']:
                    # ë‹¨ì› ë²ˆí˜¸ê°€ ë†’ìœ¼ë©´ ê³ ê¸‰ ê°œë…
                    if unit_number and str(unit_number).isdigit():
                        if int(unit_number) > 5:  # ë‹¨ì› ë²ˆí˜¸ê°€ 5ë³´ë‹¤ í¬ë©´ ê³ ê¸‰
                            advanced_terms.append(concept)
                            break
                
                # ë‚´ìš©ì—ì„œ ê³ ê¸‰ ì§€í‘œ í™•ì¸
                if any(indicator in content for indicator in advanced_indicators['content_keywords']):
                    advanced_terms.append(concept)
                    break
            
            # 4. ê°œë… ê´€ê³„ì—ì„œ ê³ ê¸‰ ê°œë… í™•ì¸
            if concept in self._concept_relations:
                related_concepts = self._concept_relations[concept]
                # ê´€ë ¨ ê°œë… ì¤‘ ê³ ê¸‰ ê°œë…ì´ ìˆìœ¼ë©´ ë³¸ ê°œë…ë„ ê³ ê¸‰
                for related in related_concepts:
                    if any(advanced in related for advanced in ['ë¯¸ë¶„', 'ì ë¶„', 'ë¯¸ì ë¶„', 'ê¸°í•˜', 'í™•ë¥ ', 'í†µê³„']):
                        advanced_terms.append(concept)
                        break
            
            conn.close()
            
        except Exception as e:
            logger.error(f"ê³ ê¸‰ ìš©ì–´ ì‹ë³„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return list(set(advanced_terms))
    
    def _generate_recommendations_with_context(self, suggested_terms: List[str], concept_level: str, achievement_standards: List[str], teaching_notes: List[str], textbook_examples: List[str]) -> List[str]:
        """ìš©ì–´ ì‚¬ìš© ê¶Œì¥ì‚¬í•­ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)"""
        recommendations = []
        
        if suggested_terms:
            recommendations.append(f"ì œì•ˆëœ ìš©ì–´: {', '.join(suggested_terms[:5])}")
        else:
            recommendations.append("ì œì•ˆëœ ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        recommendations.append(f"ì ì • ìˆ˜ì¤€: {concept_level}")
        recommendations.append("ìš©ì–´ ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­:")
        recommendations.append("- í•™ìƒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…")
        recommendations.append("- êµê³¼ì„œì—ì„œ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ ìš©ì–´ ìš°ì„  ì‚¬ìš©")
        recommendations.append("- ë³µì¡í•œ ê°œë…ì€ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ì—¬ ì„¤ëª…")
        
        # ì„±ì·¨ê¸°ì¤€ ì¶”ê°€
        if achievement_standards:
            recommendations.append("")
            recommendations.append("ğŸ“š ì„±ì·¨ê¸°ì¤€:")
            for i, std in enumerate(achievement_standards[:3], 1):
                # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ì˜ë¼ì„œ í‘œì‹œ
                content = std[:200] + "..." if len(std) > 200 else std
                recommendations.append(f"{i}. {content}")
        
        # ìœ ì˜ì‚¬í•­ ì¶”ê°€
        if teaching_notes:
            recommendations.append("")
            recommendations.append("âš ï¸ ìœ ì˜ì‚¬í•­:")
            for i, note in enumerate(teaching_notes[:3], 1):
                content = note[:150] + "..." if len(note) > 150 else note
                recommendations.append(f"{i}. {content}")
        
        # êµê³¼ì„œ ì˜ˆì‹œ ì¶”ê°€
        if textbook_examples:
            recommendations.append("")
            recommendations.append("ğŸ“– êµê³¼ì„œ ì˜ˆì‹œ:")
            for i, example in enumerate(textbook_examples[:3], 1):
                content = example[:180] + "..." if len(example) > 180 else example
                recommendations.append(f"{i}. {content}")
        
        return recommendations
    
    def _extract_math_terms(self, content: str) -> List[str]:
        """ë‚´ìš©ì—ì„œ ìˆ˜í•™ ìš©ì–´ ì¶”ì¶œ"""
        # ìˆ˜í•™ ìš©ì–´ íŒ¨í„´ (ë” í¬ê´„ì ìœ¼ë¡œ)
        math_terms = re.findall(r'[ê°€-í£]+(?:í•¨ìˆ˜|ìˆ˜ì—´|í™•ë¥ |í†µê³„|ê¸°í•˜|ë¯¸ë¶„|ì ë¶„|ë°©ì •ì‹|ë¶€ë“±ì‹|ì§‘í•©|ëª…ì œ|ë²¡í„°|í–‰ë ¬|ê·¹í•œ|ì—°ì†|ë„í•¨ìˆ˜|ì •ì ë¶„|ë¶€ì •ì ë¶„|ë“±ì°¨ìˆ˜ì—´|ë“±ë¹„ìˆ˜ì—´|ê·€ë‚©ë²•|ì‚¬ì¸ë²•ì¹™|ì½”ì‚¬ì¸ë²•ì¹™|ì‚¼ê°í˜•|ì›|í¬ë¬¼ì„ |íƒ€ì›|ìŒê³¡ì„ )', content)
        
        # ê¸°ë³¸ ìˆ˜í•™ ìš©ì–´ë„ í¬í•¨
        basic_terms = re.findall(r'\b(?:í•¨ìˆ˜|ìˆ˜ì—´|í™•ë¥ |í†µê³„|ê¸°í•˜|ë¯¸ë¶„|ì ë¶„|ë°©ì •ì‹|ë¶€ë“±ì‹|ì§‘í•©|ëª…ì œ|ë²¡í„°|í–‰ë ¬|ê·¹í•œ|ì—°ì†|ë„í•¨ìˆ˜|ì •ì ë¶„|ë¶€ì •ì ë¶„|ë“±ì°¨ìˆ˜ì—´|ë“±ë¹„ìˆ˜ì—´|ê·€ë‚©ë²•|ì‚¬ì¸ë²•ì¹™|ì½”ì‚¬ì¸ë²•ì¹™|ì‚¼ê°í˜•|ì›|í¬ë¬¼ì„ |íƒ€ì›|ìŒê³¡ì„ )\b', content)
        
        # ê³ ê¸‰ ìš©ì–´ë„ í¬í•¨
        advanced_terms = re.findall(r'\b(?:ë„í•¨ìˆ˜|ì •ì ë¶„|ë¶€ì •ì ë¶„|í•©ì„±í•¨ìˆ˜|ì—­í•¨ìˆ˜|ì¼ëŒ€ì¼í•¨ìˆ˜|ì í™”ì‹|ë¬´í•œê¸‰ìˆ˜|ì¡°ê±´ë¶€í™•ë¥ |ë² ì´ì¦ˆ ì •ë¦¬|í™•ë¥ ë¶„í¬|í‘œë³¸ë¶„ì‚°|ì‹ ë¢°êµ¬ê°„|ê°€ì„¤ê²€ì •)\b', content)
        
        all_terms = math_terms + basic_terms + advanced_terms
        return list(set(all_terms))
    
    async def _verify_single_term(self, term: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ìš©ì–´ ê²€ì¦"""
        try:
            # ìš©ì–´ ê²€ìƒ‰ìœ¼ë¡œ êµê³¼ê³¼ì • ì í•©ì„± í™•ì¸
            search_result = await self.search(term, k=2)
            
            if "error" in search_result:
                return {
                    "is_appropriate": False,
                    "issue": "ìš©ì–´ ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "suggestion": term
                }
            
            # êµê³¼ê³¼ì • ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì ì ˆí•œ ìš©ì–´
            curriculum_count = len(search_result.get("curriculum_results", []))
            textbook_count = len(search_result.get("textbook_results", []))
            
            # ê³ ê¸‰ ìš©ì–´ ëª©ë¡ í™•ì¸
            advanced_terms = []
            for concept, terms in {
                "ë¯¸ë¶„": ["ë„í•¨ìˆ˜", "ë¯¸ë¶„ê³„ìˆ˜", "ì—°ì‡„ë²•ì¹™", "í¸ë¯¸ë¶„", "ì „ë¯¸ë¶„"],
                "ì ë¶„": ["ì •ì ë¶„", "ë¶€ì •ì ë¶„", "ì¹˜í™˜ì ë¶„", "ë¶€ë¶„ì ë¶„", "ì´ì¤‘ì ë¶„", "ì‚¼ì¤‘ì ë¶„"],
                "í•¨ìˆ˜": ["í•©ì„±í•¨ìˆ˜", "ì—­í•¨ìˆ˜", "ì¼ëŒ€ì¼í•¨ìˆ˜", "ì „ì‚¬í•¨ìˆ˜", "ë‹¨ì‚¬í•¨ìˆ˜"],
                "ìˆ˜ì—´": ["ì í™”ì‹", "ìˆ˜í•™ì  ê·€ë‚©ë²•", "ë¬´í•œê¸‰ìˆ˜", "ê¸‰ìˆ˜ì˜ ìˆ˜ë ´", "ê¸‰ìˆ˜ì˜ ë°œì‚°"],
                "í™•ë¥ ": ["ì¡°ê±´ë¶€í™•ë¥ ", "ë² ì´ì¦ˆ ì •ë¦¬", "í™•ë¥ ë¶„í¬", "ê¸°ëŒ“ê°’", "ë¶„ì‚°", "í‘œì¤€í¸ì°¨"],
                "í†µê³„": ["í‘œë³¸ë¶„ì‚°", "ì‹ ë¢°êµ¬ê°„", "ê°€ì„¤ê²€ì •", "íšŒê·€ë¶„ì„", "ìƒê´€ë¶„ì„"]
            }.items():
                if term in terms:
                    advanced_terms.append(concept)
            
            # ê³ ê¸‰ ìš©ì–´ì´ë©´ì„œ êµê³¼ê³¼ì •ì— ì—†ëŠ” ê²½ìš° ë¶€ì ì ˆ
            if advanced_terms and curriculum_count == 0:
                alternative = self._suggest_alternative_term(term)
                return {
                    "is_appropriate": False,
                    "issue": f"ê³ ê¸‰ ìš©ì–´ì…ë‹ˆë‹¤. {', '.join(advanced_terms)} ìˆ˜ì¤€ì—ì„œ ë‹¤ë£¨ì–´ì§‘ë‹ˆë‹¤.",
                    "suggestion": alternative
                }
            
            if curriculum_count > 0 or textbook_count > 0:
                return {
                    "is_appropriate": True,
                    "issue": None,
                    "suggestion": term
                }
            else:
                # ëŒ€ì•ˆ ìš©ì–´ ì œì•ˆ
                alternative = self._suggest_alternative_term(term)
                return {
                    "is_appropriate": False,
                    "issue": "êµê³¼ê³¼ì •ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ìš©ì–´ì…ë‹ˆë‹¤.",
                    "suggestion": alternative
                }
                
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ìš©ì–´ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {
                "is_appropriate": False,
                "issue": f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}",
                "suggestion": term
            }
    
    def _suggest_alternative_term(self, term: str) -> str:
        """ëŒ€ì•ˆ ìš©ì–´ ì œì•ˆ"""
        alternatives = {
            "ë„í•¨ìˆ˜": "ë¯¸ë¶„ê³„ìˆ˜",
            "ì •ì ë¶„": "ì ë¶„",
            "ë¶€ì •ì ë¶„": "ì ë¶„",
            "í•©ì„±í•¨ìˆ˜": "í•¨ìˆ˜",
            "ì—­í•¨ìˆ˜": "í•¨ìˆ˜",
            "ì¼ëŒ€ì¼í•¨ìˆ˜": "í•¨ìˆ˜",
            "ì í™”ì‹": "ìˆ˜ì—´",
            "ë¬´í•œê¸‰ìˆ˜": "ìˆ˜ì—´ì˜ í•©",
            "ì¡°ê±´ë¶€í™•ë¥ ": "í™•ë¥ ",
            "ë² ì´ì¦ˆ ì •ë¦¬": "í™•ë¥ ",
            "í™•ë¥ ë¶„í¬": "í™•ë¥ ",
            "í‘œë³¸ë¶„ì‚°": "ë¶„ì‚°",
            "ì‹ ë¢°êµ¬ê°„": "í†µê³„",
            "ê°€ì„¤ê²€ì •": "í†µê³„",
            "í¸ë¯¸ë¶„": "ë¯¸ë¶„",
            "ì „ë¯¸ë¶„": "ë¯¸ë¶„",
            "ì¹˜í™˜ì ë¶„": "ì ë¶„",
            "ë¶€ë¶„ì ë¶„": "ì ë¶„",
            "ì´ì¤‘ì ë¶„": "ì ë¶„",
            "ì‚¼ì¤‘ì ë¶„": "ì ë¶„",
            "ì „ì‚¬í•¨ìˆ˜": "í•¨ìˆ˜",
            "ë‹¨ì‚¬í•¨ìˆ˜": "í•¨ìˆ˜",
            "ê¸‰ìˆ˜ì˜ ìˆ˜ë ´": "ìˆ˜ì—´",
            "ê¸‰ìˆ˜ì˜ ë°œì‚°": "ìˆ˜ì—´",
            "ê¸°ëŒ“ê°’": "í‰ê· ",
            "ë¶„ì‚°": "ë¶„ì‚°",
            "í‘œì¤€í¸ì°¨": "í‘œì¤€í¸ì°¨",
            "íšŒê·€ë¶„ì„": "í†µê³„",
            "ìƒê´€ë¶„ì„": "í†µê³„",
            "ì—­ì‚¼ê°í•¨ìˆ˜": "ì‚¼ê°í•¨ìˆ˜",
            "ìŒê³¡ì„ í•¨ìˆ˜": "ì‚¼ê°í•¨ìˆ˜",
            "ë³µì†Œì‚¼ê°í•¨ìˆ˜": "ì‚¼ê°í•¨ìˆ˜"
        }
        
        return alternatives.get(term, term)

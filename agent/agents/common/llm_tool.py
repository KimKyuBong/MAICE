"""
LLM í˜¸ì¶œì„ ìœ„í•œ ê³µí†µ íˆ´
- ë‹¤ì–‘í•œ LLM ëª¨ë¸ ì§€ì›
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬
- ì„¤ì • ì¤‘ì•™í™”
- ì—ëŸ¬ ì²˜ë¦¬ í†µì¼
"""

import asyncio
import logging
import os
import tiktoken
import json
import uuid
from typing import Dict, Any, List, Optional, Union
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from datetime import datetime
import asyncpg

# chat_completionì€ ì´ íŒŒì¼ì— ì§ì ‘ êµ¬í˜„ë¨
from agents.base_agent import Tool

# ì¶”ê°€ import for multi-provider support
try:
    import anthropic
except ImportError:
    anthropic = None

try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    import httpx
    import aiohttp
    import json
except ImportError:
    httpx = None
    json = None

logger = logging.getLogger(__name__)


class LLMProvider(Enum):
    """ì§€ì›í•˜ëŠ” LLM í”„ë¡œë°”ì´ë”"""

    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    MCP = "mcp"


# chat_completion í•¨ìˆ˜ ì¶”ê°€ (ê¸°ì¡´ llm.pyì—ì„œ ì´ë™)
async def chat_completion(
    messages: List[Dict[str, Any]],
    *,
    model: Optional[str] = None,
    max_completion_tokens: int = 1000,
    stream: bool = False,
    **kwargs,
):
    """LLM í˜¸ì¶œ í•¨ìˆ˜ - OpenAI ì§ì ‘ í˜¸ì¶œ"""
    try:
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        model_name = model or os.getenv("OPENAI_CHAT_MODEL", "gpt-5-mini")

        # GPT-5-miniëŠ” temperature íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°
        api_params = {
            "model": model_name,
            "messages": messages,
            "max_completion_tokens": max_completion_tokens,
            "stream": stream,
        }

        # temperatureëŠ” ê¸°ë³¸ê°’(1)ë§Œ ì§€ì›í•˜ë¯€ë¡œ ì œê±°
        # temperature=kwargs.get("temperature", 0.7)  # GPT-5-miniì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ

        response = await client.chat.completions.create(**api_params)

        return response

    except Exception as e:
        logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise


def count_tokens(text: str, model: str = "gpt-4") -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        # modelì´ Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not model or model.strip() == "":
            model = "gpt-4"

        model = str(model).lower()

        # MCP/penGPT ëª¨ë¸ì˜ ê²½ìš° ê¸°ë³¸ cl100k_base ì‚¬ìš©
        if "pengpt" in model or "mcp" in model or "pen" in model.lower():
            encoding = tiktoken.get_encoding("cl100k_base")
        # gpt-5 ì‹œë¦¬ì¦ˆëŠ” cl100k_base ì¸ì½”ë”© ì‚¬ìš©
        elif model in ["gpt-5-mini", "gpt-5-nano", "gpt-5"]:
            encoding = tiktoken.get_encoding("cl100k_base")
        # Claude ëª¨ë¸ë“¤ì€ cl100k_base ì‚¬ìš©
        elif model.startswith("claude"):
            encoding = tiktoken.get_encoding("cl100k_base")
        # Gemini ëª¨ë¸ë“¤ì€ ëŒ€ëµì ì¸ ê³„ì‚° (tiktokenì´ ì§€ì›í•˜ì§€ ì•ŠìŒ)
        elif model.startswith("gemini"):
            return len(text) // 4  # ëŒ€ëµì ì¸ ê³„ì‚°
        else:
            try:
                encoding = tiktoken.get_encoding(
                    "cl100k_base"
                )  # ê¸°ë³¸ê°’ìœ¼ë¡œ cl100k_base ì‚¬ìš©
            except:
                # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš° UTF-8 ë°”ì´íŠ¸ ê¸¸ì´ ê¸°ì¤€ ì¶”ì •
                return len(text.encode("utf-8")) // 3

        return len(encoding.encode(text))
    except Exception as e:
        logger.warning(f"í† í° ê³„ì‚° ì‹¤íŒ¨: {e}")
        # ëŒ€ëµì ì¸ ì¶”ì • (ì˜ì–´ ê¸°ì¤€ 4ìë‹¹ 1í† í°)
        return len(text) // 4


def count_messages_tokens(messages: List[Dict[str, str]], model: str = "gpt-4") -> int:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    total_tokens = 0
    for message in messages:
        content = message.get("content", "")
        total_tokens += count_tokens(content, model)
    return total_tokens


@dataclass
class LLMConfig:
    """LLM ì„¤ì •"""

    provider: LLMProvider = LLMProvider(os.getenv("LLM_PROVIDER", "openai"))
    model: str = ""
    max_tokens: int = 1000
    temperature: Optional[float] = None
    stream: bool = False
    timeout: int = 120
    max_retries: int = 3
    retry_delay: float = 2.0
    json_response: bool = False

    def __post_init__(self):
        """í”„ë¡œë°”ì´ë”ì— ë”°ë¼ ê¸°ë³¸ ëª¨ë¸ ì„¤ì •"""
        if not self.model:
            if self.provider == LLMProvider.OPENAI:
                self.model = os.getenv("OPENAI_CHAT_MODEL", "gpt-5-mini")
            elif self.provider == LLMProvider.ANTHROPIC:
                self.model = os.getenv(
                    "ANTHROPIC_CHAT_MODEL", "claude-sonnet-4-20250514"
                )
            elif self.provider == LLMProvider.GOOGLE:
                self.model = os.getenv("GOOGLE_CHAT_MODEL", "gemini-2.5-flash-lite")
            elif self.provider == LLMProvider.MCP:
                self.model = os.getenv("MCP_MODEL", "penGPT")


@dataclass
class PromptTemplate:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""

    system_prompt: str
    user_template: str
    variables: Dict[str, Any] = None


class LLMTool(Tool):
    """LLM í˜¸ì¶œì„ ìœ„í•œ ê³µí†µ íˆ´"""

    def __init__(
        self,
        name: str = "llm_tool",
        config: LLMConfig = None,
        prompt_template: PromptTemplate = None,
    ):
        super().__init__(name)
        self.config = config or LLMConfig()
        self.prompt_template = prompt_template
        self.logger = logging.getLogger(f"tools.{name}")

        # DB ì—°ê²° ì •ë³´ ì„¤ì •
        self.db_url = os.getenv(
            "AGENT_DATABASE_URL",
            "postgresql://postgres:postgres@localhost:5432/maice_agent",
        )

    async def _save_prompt_to_db(
        self,
        messages: List[Dict[str, str]],
        config: LLMConfig,
        prompt_input: Union[str, PromptTemplate, Dict, None] = None,
        variables: Dict[str, Any] = None,
        message_id: str = None,
        session_id: int = None,
        user_id: int = None,
        input_tokens: int = None,
    ):
        """í”„ë¡¬í”„íŠ¸ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ - maice_agent DB ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜

        Returns:
            int: ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ì˜ ID
        """
        try:
            conn = await asyncpg.connect(self.db_url)

            # message_idê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ìƒì„±
            if not message_id:
                message_id = str(uuid.uuid4())

            # í”„ë¡¬í”„íŠ¸ ì „ì²´ ë‚´ìš© êµ¬ì„±
            prompt_content = json.dumps(
                {
                    "messages": messages,
                    "prompt_input": str(prompt_input) if prompt_input else None,
                    "variables": variables,
                    "config": {
                        "provider": config.provider.value,
                        "model": config.model,
                        "max_tokens": config.max_tokens,
                        "stream": config.stream,
                        "temperature": config.temperature,
                    },
                },
                ensure_ascii=False,
            )

            # llm_prompt_logs í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë§ì¶¤ (ì›ê²© DB) - ID ë°˜í™˜
            prompt_id = await conn.fetchval(
                """
                INSERT INTO llm_prompt_logs (
                    tool_name, provider, model, session_id, user_id, agent_name,
                    input_prompt, input_tokens, max_tokens, temperature, stream,
                    timeout, max_retries, created_at
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
                RETURNING id
            """,
                self.name,  # tool_name (NOT NULL)
                config.provider.value,  # provider (NOT NULL)
                config.model,  # model (NOT NULL)
                session_id or 0,  # session_id
                user_id or 0,  # user_id
                self.name,  # agent_name
                prompt_content,  # input_prompt
                input_tokens or 0,  # input_tokens
                config.max_tokens,  # max_tokens
                float(config.temperature)
                if config.temperature is not None
                else None,  # temperature
                config.stream,  # stream
                config.timeout,  # timeout
                config.max_retries,  # max_retries
                datetime.now(),  # created_at
            )

            await conn.close()
            self.logger.info(
                f"âœ… í”„ë¡¬í”„íŠ¸ DB ì €ì¥ ì™„ë£Œ: ID={prompt_id}, ì„¸ì…˜ {session_id}, {self.name}"
            )
            return prompt_id

        except Exception as e:
            self.logger.error(f"âŒ í”„ë¡¬í”„íŠ¸ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback

            self.logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            # DB ì €ì¥ ì‹¤íŒ¨í•´ë„ LLM í˜¸ì¶œì€ ê³„ì† ì§„í–‰
            return None

    async def _save_response_to_db(
        self,
        response_content: str,
        config: LLMConfig,
        input_tokens: int,
        response_time: float = None,
        message_id: str = None,
        session_id: int = None,
        user_id: int = None,
        prompt_id: int = None,
    ):
        """ì‘ë‹µì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ - maice_agent DB ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜

        Args:
            response_content: ì‘ë‹µ ë‚´ìš©
            config: LLM ì„¤ì •
            input_tokens: ì…ë ¥ í† í° ìˆ˜
            response_time: ì‘ë‹µ ì‹œê°„ (ì´ˆ)
            message_id: ë©”ì‹œì§€ ID
            session_id: ì„¸ì…˜ ID
            user_id: ì‚¬ìš©ì ID
            prompt_id: í”„ë¡¬í”„íŠ¸ ID (í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ ì—°ê²°)
        """
        try:
            conn = await asyncpg.connect(self.db_url)

            # ì‘ë‹µ ë°ì´í„° ì‚½ì… (ì›ê²© DB ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜) - prompt_id í¬í•¨
            await conn.execute(
                """
                INSERT INTO llm_response_logs (
                    tool_name, provider, model, prompt_id, session_id, user_id, agent_name, 
                    response_content, response_tokens, response_time_ms, success, created_at
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
            """,
                self.name,  # tool_name (NOT NULL)
                config.provider.value,  # provider (NOT NULL)
                config.model,  # model (NOT NULL)
                prompt_id,  # prompt_id - í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ ì—°ê²°
                session_id or 0,  # session_id
                user_id or 0,  # user_id
                self.name,  # agent_name
                response_content,  # response_content
                count_tokens(response_content, config.model),  # response_tokens
                int(response_time * 1000) if response_time else 0,  # response_time_ms
                True,  # success
                datetime.now(),  # created_at
            )

            await conn.close()
            self.logger.info(
                f"âœ… ì‘ë‹µ DB ì €ì¥ ì™„ë£Œ: ì„¸ì…˜ {session_id}, í”„ë¡¬í”„íŠ¸ ID={prompt_id}, {self.name}"
            )

        except Exception as e:
            self.logger.error(f"âŒ ì‘ë‹µ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback

            self.logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            # DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    async def execute(
        self,
        prompt: Union[str, PromptTemplate] = None,
        variables: Dict[str, Any] = None,
        config_override: LLMConfig = None,
        message_id: str = None,
        session_id: int = None,
        streams_client=None,
        request_id: str = None,
        json_response: bool = False,
        **kwargs,
    ) -> Dict[str, Any]:
        # request_idë¥¼ ì €ì¥í•´ë‘ì–´ì„œ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì—ë„ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ í•¨
        self._current_request_id = request_id
        """
        LLM í˜¸ì¶œ ì‹¤í–‰
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ ë˜ëŠ” í…œí”Œë¦¿
            variables: í…œí”Œë¦¿ ë³€ìˆ˜
            config_override: ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
        
        Returns:
            Dict[str, Any]: LLM ì‘ë‹µ ê²°ê³¼
        """
        try:
            # json_response íŒŒë¼ë¯¸í„°ë¥¼ config_overrideì— ë°˜ì˜
            if json_response:
                if config_override is None:
                    config_override = LLMConfig()
                config_override.json_response = json_response

            # ì„¤ì • ë³‘í•©
            config = self._merge_config(config_override)

            # í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
            messages = self._prepare_messages(prompt, variables)

            # í† í° ìˆ˜ ê³„ì‚° ë° ë¡œê·¸
            input_tokens = count_messages_tokens(messages, config.model)
            self.logger.debug(
                f"ğŸ“Š ì…ë ¥ í† í° ìˆ˜: {input_tokens} (ì œí•œ: {config.max_tokens})"
            )

            # í”„ë¡¬í”„íŠ¸ ìš”ì•½ ì •ë³´ë§Œ ë¡œê¹…
            # if isinstance(prompt, dict):
            #     self.logger.info(f"ğŸš€ LLM í˜¸ì¶œ ì‹œì‘ - system: {len(prompt.get('system', ''))}ì, user: {len(prompt.get('user', ''))}ì")
            # else:
            #     self.logger.info(f"ğŸš€ LLM í˜¸ì¶œ ì‹œì‘ - í”„ë¡¬í”„íŠ¸: {len(str(prompt))}ì")

            # í”„ë¡¬í”„íŠ¸ë¥¼ DBì— ì €ì¥ (ì„¸ì…˜ ID, ì‚¬ìš©ì ID í¬í•¨) - ID ë°˜í™˜ë°›ìŒ
            prompt_id = await self._save_prompt_to_db(
                messages,
                config,
                prompt,
                variables,
                message_id,
                session_id,
                kwargs.get("user_id"),
                input_tokens,
            )

            # LLM í˜¸ì¶œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            start_time = asyncio.get_event_loop().time()
            response = await self._call_llm_with_retry(messages, config)
            end_time = asyncio.get_event_loop().time()
            response_time = end_time - start_time

            # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            if config.stream:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… content ë°˜í™˜ (ì‹¤ì‹œê°„ ì²­í¬ ì „ì†¡ í¬í•¨)
                full_content = await self._process_streaming_response(
                    response, config, session_id, streams_client
                )

                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì‘ë‹µì„ DBì— ì €ì¥ - prompt_id ì „ë‹¬í•˜ì—¬ ì—°ê²°
                await self._save_response_to_db(
                    full_content,
                    config,
                    input_tokens,
                    response_time,
                    message_id,
                    session_id,
                    kwargs.get("user_id"),
                    prompt_id,
                )

                return {
                    "success": True,
                    "content": full_content,
                    "usage": None,
                    "model": config.model,
                    "provider": config.provider.value,
                }
            else:
                # MCP í”„ë¡œë°”ì´ë”ì˜ ê²½ìš° contentê°€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ
                content = response.choices[0].message.content
                if isinstance(content, list):
                    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œì˜ text ì¶”ì¶œ
                    if (
                        len(content) > 0
                        and isinstance(content[0], dict)
                        and "text" in content[0]
                    ):
                        content = content[0]["text"]
                    else:
                        content = str(content[0]) if content else ""
                elif not isinstance(content, str):
                    content = str(content)

                # ì‘ë‹µì„ DBì— ì €ì¥ (ì„¸ì…˜ ID, ì‚¬ìš©ì ID í¬í•¨) - prompt_id ì „ë‹¬í•˜ì—¬ ì—°ê²°
                await self._save_response_to_db(
                    content,
                    config,
                    input_tokens,
                    response_time,
                    message_id,
                    session_id,
                    kwargs.get("user_id"),
                    prompt_id,
                )

                # LLM í˜¸ì¶œ ì™„ë£Œ ë¡œê¹…
                self.logger.info(
                    f"âœ… LLM í˜¸ì¶œ ì™„ë£Œ - ì‘ë‹µ: {len(content)}ì, ì†Œìš”ì‹œê°„: {response_time:.2f}ì´ˆ"
                )

                return {
                    "success": True,
                    "content": content,
                    "usage": getattr(response, "usage", None),
                    "model": config.model,
                    "provider": config.provider.value,
                }

        except Exception as e:
            self.logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "content": None}

    def _merge_config(self, override: Optional[LLMConfig]) -> LLMConfig:
        """ì„¤ì • ë³‘í•©"""
        if not override:
            return self.config

        # ê¸°ì¡´ ì„¤ì •ì„ ë³µì‚¬í•˜ê³  ì˜¤ë²„ë¼ì´ë“œ ì ìš©
        merged = LLMConfig(
            provider=override.provider or self.config.provider,
            model=override.model or self.config.model,
            max_tokens=override.max_tokens or self.config.max_tokens,
            temperature=override.temperature
            if override.temperature is not None
            else self.config.temperature,
            stream=override.stream
            if override.stream is not None
            else self.config.stream,
            timeout=override.timeout or self.config.timeout,
            max_retries=override.max_retries or self.config.max_retries,
            retry_delay=override.retry_delay or self.config.retry_delay,
            json_response=override.json_response
            if override.json_response is not None
            else self.config.json_response,
        )
        return merged

    def _prepare_messages(
        self,
        prompt: Union[str, PromptTemplate, Dict, None],
        variables: Dict[str, Any] = None,
    ) -> List[Dict[str, str]]:
        """ë©”ì‹œì§€ ì¤€ë¹„"""
        if isinstance(prompt, PromptTemplate):
            # í…œí”Œë¦¿ ì‚¬ìš©
            system_prompt = self._format_template(prompt.system_prompt, variables or {})
            user_content = self._format_template(prompt.user_template, variables or {})
        elif isinstance(prompt, dict):
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ (PromptBuilderV2 ê²°ê³¼)
            system_prompt = prompt.get("system", "")
            user_content = prompt.get("user", "")
        elif isinstance(prompt, str):
            # ë‹¨ìˆœ ë¬¸ìì—´
            system_prompt = (
                self.prompt_template.system_prompt if self.prompt_template else ""
            )
            user_content = prompt
        else:
            # ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©
            if not self.prompt_template:
                raise ValueError("í”„ë¡¬í”„íŠ¸ë‚˜ í…œí”Œë¦¿ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            system_prompt = self.prompt_template.system_prompt
            user_content = self._format_template(
                self.prompt_template.user_template, variables or {}
            )

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_content})

        return messages

    def _format_template(self, template: str, variables: Dict[str, Any]) -> str:
        """ìŠ¤ë§ˆíŠ¸ í…œí”Œë¦¿ í¬ë§·íŒ… - ëˆ„ë½ëœ ë³€ìˆ˜ ìë™ ê°ì§€ ë° ì²˜ë¦¬"""
        if not template:
            return template

        if not variables:
            return template

        # 1. í…œí”Œë¦¿ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ë“¤ ìë™ ì¶”ì¶œ
        import re

        template_vars = set(re.findall(r"\{([^}]+)\}", template))
        provided_vars = set(variables.keys())
        missing_vars = template_vars - provided_vars

        # 2. ëˆ„ë½ëœ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ê¸°ë³¸ê°’ ìƒì„±
        smart_variables = variables.copy()
        for var in missing_vars:
            smart_variables[var] = self._generate_smart_default(var, template)
            self.logger.info(f"ìŠ¤ë§ˆíŠ¸ ë³€ìˆ˜ ìƒì„±: {var} = {smart_variables[var]}")

        try:
            # 3. ì•ˆì „í•œ í…œí”Œë¦¿ ì¹˜í™˜
            result = template.format(**smart_variables)

            # 4. JSON í˜•ì‹ ìš”êµ¬ì‚¬í•­ ë³´ì¡´ í™•ì¸
            if (
                "JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”" not in result
                and "JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”" in template
            ):
                result = self._restore_json_requirements(template, result)

            return result

        except Exception as e:
            self.logger.error(f"í…œí”Œë¦¿ í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
            # í´ë°±: ë‹¨ìˆœ ì¹˜í™˜ ë°©ì‹
            result = template
            for key, value in smart_variables.items():
                result = result.replace(f"{{{key}}}", str(value))
            return result

    def _generate_smart_default(self, var_name: str, template: str) -> str:
        """ë³€ìˆ˜ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸í•œ ê¸°ë³¸ê°’ ìƒì„±"""
        var_lower = var_name.lower()

        # JSON ì‘ë‹µ í˜•ì‹ ë³€ìˆ˜ë“¤
        if "knowledge_code" in var_lower:
            return "K1|K2|K3|K4 ì¤‘ ì„ íƒ"
        elif "quality" in var_lower:
            return "answerable|needs_clarify|unanswerable ì¤‘ ì„ íƒ"
        elif "missing_fields" in var_lower:
            return "ì‹¤ì œ ëˆ„ë½ëœ ì •ë³´ë“¤"
        elif "unit_tags" in var_lower:
            return "ì‹¤ì œ ë‹¨ì› íƒœê·¸ë“¤"
        elif "policy_flags" in var_lower:
            return "ìœ„ë°˜ ì‚¬í•­"
        elif "reasoning" in var_lower:
            return "ì‹¤ì œ ë¶„ë¥˜ ê·¼ê±°"
        elif "clarification_questions" in var_lower:
            return "ê°€ì¥ ì¤‘ìš”í•œ ëª…ë£Œí™” ì§ˆë¬¸ 1ê°œë§Œ"
        elif "clarification_reasoning" in var_lower:
            return "ì„ íƒí•œ ëª…ë£Œí™” ì§ˆë¬¸ì´ í•´ë‹¹ K1~K4 ìœ í˜•ì˜ íŠ¹ì„±ê³¼ missing_fieldsë¥¼ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ì§€ì— ëŒ€í•œ ê·¼ê±°"
        elif "unanswerable_response" in var_lower:
            return "unanswerableì¸ ê²½ìš° ì ì ˆí•œ ì•ˆë‚´ ë©”ì‹œì§€ (ìˆ˜í•™ ì™¸ ì˜ì—­, í‰ê°€ìœ¤ë¦¬, êµê³¼ ë¶ˆì¼ì¹˜ì— ë”°ë¼ êµ¬ë¶„)"

        # K1-K4 ì •ì˜ ë³€ìˆ˜ë“¤
        elif "k1_definition" in var_lower:
            return "ì‚¬ì‹¤ì  ì§€ì‹ - ì •ì˜, ìš©ì–´, ê¸°í˜¸, ê³µì‹, ê°’, ë‹¨ìœ„ ë“± ê¸°ë³¸ ì‚¬ì‹¤"
        elif "k2_definition" in var_lower:
            return (
                "ê°œë…ì  ì§€ì‹ - ê°œë… ê°„ ê´€ê³„, ë¶„ë¥˜, ì›ë¦¬, ì´ë¡ , ë¹„êµ/ëŒ€ì¡°, ì˜¤ê°œë… ê²½ê³„"
            )
        elif "k3_definition" in var_lower:
            return "ì ˆì°¨ì  ì§€ì‹ - ìˆ˜í–‰ ë°©ë²•, ê¸°ìˆ , ì•Œê³ ë¦¬ì¦˜, ì ˆì°¨, ë‹¨ê³„ë³„ ê³¼ì •, ì¡°ê±´ê³¼ ì œì•½"
        elif "k4_definition" in var_lower:
            return "ë©”íƒ€ì¸ì§€ì  ì§€ì‹ - ì „ëµì  ì‚¬ê³ , ë¬¸ì œ ì ‘ê·¼ë²•, ê³„íš, ë°˜ì„±, ëŒ€ì•ˆ í•´ë²•"

        # ê²Œì´íŒ… ê¸°ì¤€ ë³€ìˆ˜ë“¤
        elif "answerable_criteria" in var_lower:
            return "êµê³¼(ìˆ˜í•™), ë‹¨ì›Â·ìˆ˜ì¤€ ì§€ì •, ëª©í‘œ ë™ì‚¬ ëª…í™•, ì¶©ë¶„í•œ ì •ë³´ ì œê³µ"
        elif "needs_clarify_criteria" in var_lower:
            return "ë²”ìœ„ ê³¼ëŒ€/ëª©í‘œ ë¶ˆëª…/ìˆ˜ì¤€ ë¶ˆëª…/ìš©ì–´ í˜¼ë™, ì¶”ê°€ ì •ë³´ í•„ìš”"
        elif "unanswerable_criteria" in var_lower:
            return "ìˆ˜í•™ ì™¸ ì˜ì—­, í‰ê°€ìœ¤ë¦¬ ìœ„ë°°, êµê³¼ ë¶ˆì¼ì¹˜ ì‹¬ê°"

        # ê¸°ë³¸ê°’
        else:
            return f"[{var_name}]"

    def _restore_json_requirements(
        self, original_template: str, current_result: str
    ) -> str:
        """JSON í˜•ì‹ ìš”êµ¬ì‚¬í•­ ë³µì›"""
        try:
            if "## ğŸš¨ ì¤‘ìš”: ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”! ğŸš¨" in original_template:
                json_section = original_template.split(
                    "## ğŸš¨ ì¤‘ìš”: ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”! ğŸš¨"
                )[1]
                if "## ğŸš¨ ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°" in json_section:
                    json_section = json_section.split("## ğŸš¨ ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°")[0]
                return (
                    current_result
                    + "\n\n"
                    + "## ğŸš¨ ì¤‘ìš”: ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”! ğŸš¨"
                    + json_section
                    + "\n\n## ğŸš¨ ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°: JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”! ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ê¸ˆì§€! ğŸš¨"
                )
        except Exception as e:
            self.logger.warning(f"JSON ìš”êµ¬ì‚¬í•­ ë³µì› ì‹¤íŒ¨: {e}")
        return current_result

    async def _call_llm_with_retry(
        self, messages: List[Dict[str, str]], config: LLMConfig
    ):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ LLM í˜¸ì¶œ"""
        last_error = None

        for attempt in range(config.max_retries):
            try:
                self.logger.info(f"LLM í˜¸ì¶œ ì‹œë„ {attempt + 1}/{config.max_retries}")
                self.logger.debug(
                    f"ğŸ”§ LLM ì„¤ì •: provider={config.provider}, model={config.model}, max_tokens={config.max_tokens}, timeout={config.timeout}s"
                )

                # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                start_time = asyncio.get_event_loop().time()

                # í”„ë¡œë°”ì´ë”ë³„ LLM í˜¸ì¶œ
                if config.provider == LLMProvider.OPENAI:
                    # OpenAI API íŒŒë¼ë¯¸í„° êµ¬ì„±
                    api_params = {
                        "messages": messages,
                        "model": config.model,
                        "max_completion_tokens": config.max_tokens,
                        "stream": config.stream,
                    }

                    if config.temperature is not None:
                        api_params["temperature"] = config.temperature

                    # JSON ì‘ë‹µ í˜•ì‹ ì¶”ê°€
                    if config.json_response:
                        api_params["response_format"] = {"type": "json_object"}

                    response = await asyncio.wait_for(
                        chat_completion(**api_params),
                        timeout=config.timeout,
                    )
                elif config.provider == LLMProvider.ANTHROPIC and anthropic:
                    # Anthropic Claude í˜¸ì¶œ
                    client = anthropic.AsyncAnthropic(
                        api_key=os.getenv("ANTHROPIC_API_KEY")
                    )

                    # system ë©”ì‹œì§€ì™€ user ë©”ì‹œì§€ ë¶„ë¦¬
                    system_message = None
                    user_messages = []

                    for msg in messages:
                        if msg["role"] == "system":
                            system_message = msg["content"]
                        else:
                            user_messages.append(msg)

                    # API í˜¸ì¶œ íŒŒë¼ë¯¸í„° êµ¬ì„±
                    api_params = {
                        "model": config.model,
                        "max_tokens": config.max_tokens,
                        "messages": user_messages,
                        "stream": config.stream,  # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì • ì¶”ê°€
                        **(
                            {}
                            if config.temperature is None
                            else {"temperature": config.temperature}
                        ),
                    }

                    # system ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ë³„ë„ íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€
                    if system_message:
                        api_params["system"] = system_message

                    if config.stream:
                        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                        response = await asyncio.wait_for(
                            client.messages.create(**api_params), timeout=config.timeout
                        )

                        # Anthropic ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        class AnthropicStreamResponse:
                            def __init__(self, anthropic_stream):
                                self.anthropic_stream = anthropic_stream
                                self._iterated = False

                            def __aiter__(self):
                                return self

                            async def __anext__(self):
                                try:
                                    chunk = await self.anthropic_stream.__anext__()

                                    # ëª¨ë“  Anthropic ì´ë²¤íŠ¸ íƒ€ì… ë¡œê¹…
                                    chunk_type = getattr(chunk, "type", "unknown")
                                    logger.debug(
                                        f"ğŸ” Anthropic ì²­í¬ íƒ€ì…: {chunk_type}"
                                    )

                                    # Anthropic ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                    if (
                                        hasattr(chunk, "type")
                                        and chunk.type == "content_block_delta"
                                    ):
                                        # Anthropic content_block_deltaë¥¼ OpenAI delta í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                        class MockDelta:
                                            def __init__(self, content):
                                                self.content = content

                                        class MockChoice:
                                            def __init__(self, delta):
                                                self.delta = delta

                                        class MockChunk:
                                            def __init__(self, choice):
                                                self.choices = [choice]

                                        # Anthropicì—ì„œëŠ” delta.textê°€ ì•„ë‹ˆë¼ delta.textë¥¼ ì‚¬ìš©
                                        text_content = (
                                            chunk.delta.text
                                            if hasattr(chunk.delta, "text")
                                            else ""
                                        )
                                        delta = MockDelta(text_content)
                                        choice = MockChoice(delta)
                                        return MockChunk(choice)
                                    elif (
                                        hasattr(chunk, "type")
                                        and chunk.type == "message_delta"
                                    ):
                                        # message_delta ì´ë²¤íŠ¸ ì²˜ë¦¬ (stop_reason í¬í•¨)
                                        stop_reason = getattr(
                                            chunk.delta, "stop_reason", None
                                        )
                                        if stop_reason:
                                            logger.info(
                                                f"ğŸ›‘ Anthropic ì¢…ë£Œ ì‚¬ìœ : {stop_reason}"
                                            )
                                        # message_deltaëŠ” ê±´ë„ˆë›°ê³  ë‹¤ìŒ ì²­í¬ ëŒ€ê¸°
                                        return await self.__anext__()
                                    elif (
                                        hasattr(chunk, "type")
                                        and chunk.type == "message_stop"
                                    ):
                                        # ë©”ì‹œì§€ ì¢…ë£Œ ì‹ í˜¸
                                        logger.info(
                                            "âœ… Anthropic ìŠ¤íŠ¸ë¦¬ë° ì •ìƒ ì¢…ë£Œ (message_stop)"
                                        )
                                        raise StopAsyncIteration
                                    else:
                                        # ë‹¤ë¥¸ íƒ€ì…ì˜ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸° (content_block_start, content_block_stop ë“±)
                                        logger.debug(
                                            f"â­ï¸ Anthropic ì´ë²¤íŠ¸ ê±´ë„ˆë›°ê¸°: {chunk_type}"
                                        )
                                        return await self.__anext__()
                                except StopAsyncIteration:
                                    raise
                                except Exception as e:
                                    self.logger.error(
                                        f"Anthropic ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}"
                                    )
                                    raise StopAsyncIteration

                        response = AnthropicStreamResponse(response)
                    else:
                        # ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                        response = await asyncio.wait_for(
                            client.messages.create(**api_params), timeout=config.timeout
                        )
                elif config.provider == LLMProvider.GOOGLE and genai:
                    # Google Gemini í˜¸ì¶œ
                    self.logger.info(
                        f"ğŸš€ Google Gemini í˜¸ì¶œ ì‹œì‘: model={config.model}"
                    )
                    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
                    model = genai.GenerativeModel(config.model)
                    self.logger.info(f"âœ… Google Gemini ëª¨ë¸ ìƒì„± ì™„ë£Œ: {config.model}")

                    # Geminiìš© ë©”ì‹œì§€ í¬ë§· ë³€í™˜
                    prompt = ""
                    for msg in messages:
                        if msg["role"] == "system":
                            prompt += f"System: {msg['content']}\n\n"
                        elif msg["role"] == "user":
                            prompt += f"User: {msg['content']}\n\n"

                    if config.stream:
                        # Gemini ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ - ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° API ì‚¬ìš©
                        response_stream = model.generate_content(
                            prompt,
                            generation_config=genai.types.GenerationConfig(
                                max_output_tokens=config.max_tokens,
                                temperature=config.temperature or 0.7,
                            ),
                            stream=True,
                        )

                        # Gemini ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
                        class GeminiStreamResponse:
                            def __init__(self, gemini_stream, logger):
                                self.gemini_stream = gemini_stream
                                self.logger = logger
                                self._iterated = False
                                self._stream_iterator = None

                            def __aiter__(self):
                                return self

                            async def __anext__(self):
                                try:
                                    # ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œ ìŠ¤íŠ¸ë¦¼ ì´í„°ë ˆì´í„° ìƒì„±
                                    if self._stream_iterator is None:
                                        self._stream_iterator = iter(self.gemini_stream)

                                    # ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¤ìŒ ì²­í¬ ê°€ì ¸ì˜¤ê¸° (ë™ê¸° ì´í„°ë ˆì´í„°ë¥¼ ë¹„ë™ê¸°ë¡œ ê°ì‹¸ê¸°)
                                    chunk = next(self._stream_iterator)

                                    # Gemini ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                    if (
                                        hasattr(chunk, "candidates")
                                        and chunk.candidates
                                    ):
                                        candidate = chunk.candidates[0]
                                        if (
                                            hasattr(candidate, "content")
                                            and candidate.content
                                        ):
                                            if (
                                                hasattr(candidate.content, "parts")
                                                and candidate.content.parts
                                            ):
                                                text_content = candidate.content.parts[
                                                    0
                                                ].text
                                                # ë¹ˆ í…ìŠ¤íŠ¸ë„ ì²˜ë¦¬í•˜ì—¬ ì—°ì†ì„± ìœ ì§€
                                                text_content = text_content or ""

                                                class MockDelta:
                                                    def __init__(self, content):
                                                        self.content = content

                                                class MockChoice:
                                                    def __init__(self, delta):
                                                        self.delta = delta

                                                class MockChunk:
                                                    def __init__(self, choice):
                                                        self.choices = [choice]

                                                delta = MockDelta(text_content)
                                                choice = MockChoice(delta)
                                                return MockChunk(choice)

                                    # ë¹ˆ ì²­í¬ë„ ë°˜í™˜í•˜ì—¬ ì—°ì†ì„± ìœ ì§€
                                    class MockDelta:
                                        def __init__(self, content):
                                            self.content = content

                                    class MockChoice:
                                        def __init__(self, delta):
                                            self.delta = delta

                                    class MockChunk:
                                        def __init__(self, choice):
                                            self.choices = [choice]

                                    delta = MockDelta("")
                                    choice = MockChoice(delta)
                                    return MockChunk(choice)

                                except StopIteration:
                                    raise StopAsyncIteration
                                except Exception as e:
                                    self.logger.error(
                                        f"Gemini ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}"
                                    )
                                    raise StopAsyncIteration

                        response = GeminiStreamResponse(response_stream, self.logger)
                    else:
                        # ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                        response = await asyncio.wait_for(
                            model.generate_content_async(
                                prompt,
                                generation_config=genai.types.GenerationConfig(
                                    max_output_tokens=config.max_tokens,
                                    temperature=config.temperature or 0.7,
                                ),
                            ),
                            timeout=config.timeout,
                        )
                elif config.provider == LLMProvider.MCP:
                    # MCP ì„œë²„ í˜¸ì¶œ (penGPT) - OpenAI í˜¸í™˜ APIë§Œ ì‚¬ìš©
                    mcp_openai_base_url = os.getenv("MCP_OPENAI_BASE_URL")

                    # OpenAI í˜¸í™˜ API URLì´ ì—†ìœ¼ë©´ ê¸°ë³¸ MCP ì„œë²„ URLì˜ /v1 ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
                    if not mcp_openai_base_url:
                        base_url = os.getenv(
                            "MCP_SERVER_URL", "http://192.168.1.105:5555"
                        )
                        mcp_openai_base_url = f"{base_url.rstrip('/')}/v1"

                    # OpenAI í˜¸í™˜ API ì‚¬ìš©
                    from openai import AsyncOpenAI

                    self.logger.info(
                        f"ğŸ”— MCP OpenAI í˜¸í™˜ API ì‚¬ìš©: {mcp_openai_base_url}"
                    )
                    client = AsyncOpenAI(
                        api_key=os.getenv(
                            "MCP_API_KEY", os.getenv("OPENAI_API_KEY", "dummy-key")
                        ),
                        base_url=mcp_openai_base_url,
                    )

                    # OpenAI í˜¸í™˜ API í˜¸ì¶œ - messages ë°°ì—´ ê·¸ëŒ€ë¡œ ì „ì†¡
                    api_params = {
                        "model": config.model or os.getenv("MCP_MODEL", "penGPT"),
                        "messages": messages,  # OpenAI í‘œì¤€ í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        "max_completion_tokens": config.max_tokens,
                        "stream": config.stream,
                    }

                    if config.temperature is not None:
                        api_params["temperature"] = config.temperature

                    # JSON ì‘ë‹µ í˜•ì‹ ì¶”ê°€
                    if config.json_response:
                        api_params["response_format"] = {"type": "json_object"}

                    response = await asyncio.wait_for(
                        client.chat.completions.create(**api_params),
                        timeout=config.timeout,
                    )

                else:
                    raise Exception(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM í”„ë¡œë°”ì´ë”: {config.provider}")
                end_time = asyncio.get_event_loop().time()
                self.logger.debug(f"â±ï¸ LLM ì‘ë‹µ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")

                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì¸ì§€ í™•ì¸
                if config.stream:
                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜ (AnswerGeneratorì—ì„œ ì²˜ë¦¬)
                    self.logger.debug(f"ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°˜í™˜: {type(response)}")
                    return response

                # ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                if config.provider == LLMProvider.OPENAI:
                    if hasattr(response, "choices") and response.choices:
                        content = response.choices[0].message.content
                        output_tokens = count_tokens(content or "", config.model)
                        self.logger.info(f"ğŸ“¤ ì¶œë ¥ í† í° ìˆ˜: {output_tokens}")
                        self.logger.info(f"ğŸ“„ LLM ì‘ë‹µ ë‚´ìš©: {repr(content)}")
                    else:
                        self.logger.info(f"ğŸ“„ LLM ì‘ë‹µ íƒ€ì…: {type(response)}")
                elif config.provider == LLMProvider.ANTHROPIC:
                    if hasattr(response, "content") and response.content:
                        content = response.content[0].text
                        output_tokens = count_tokens(content or "", config.model)
                        self.logger.info(f"ğŸ“¤ ì¶œë ¥ í† í° ìˆ˜: {output_tokens}")
                        self.logger.info(f"ğŸ“„ LLM ì‘ë‹µ ë‚´ìš©: {repr(content)}")
                        # Anthropic ì‘ë‹µì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        response = type(
                            "Response",
                            (),
                            {
                                "choices": [
                                    type(
                                        "Choice",
                                        (),
                                        {
                                            "message": type(
                                                "Message", (), {"content": content}
                                            )()
                                        },
                                    )()
                                ]
                            },
                        )()
                    else:
                        self.logger.info(f"ğŸ“„ LLM ì‘ë‹µ íƒ€ì…: {type(response)}")
                elif config.provider == LLMProvider.GOOGLE:
                    self.logger.info(
                        f"ğŸ¯ Google Gemini ì‘ë‹µ ì²˜ë¦¬: model={config.model}"
                    )
                    if hasattr(response, "candidates") and response.candidates:
                        candidate = response.candidates[0]
                        if hasattr(candidate, "content") and candidate.content:
                            if (
                                hasattr(candidate.content, "parts")
                                and candidate.content.parts
                            ):
                                content = candidate.content.parts[0].text
                                output_tokens = count_tokens(
                                    content or "", config.model
                                )
                                self.logger.info(f"ğŸ“¤ ì¶œë ¥ í† í° ìˆ˜: {output_tokens}")
                                self.logger.info(
                                    f"ğŸ“„ Google Gemini ì‘ë‹µ ë‚´ìš©: {repr(content)}"
                                )
                                # Google Gemini ì‘ë‹µì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                response = type(
                                    "Response",
                                    (),
                                    {
                                        "choices": [
                                            type(
                                                "Choice",
                                                (),
                                                {
                                                    "message": type(
                                                        "Message",
                                                        (),
                                                        {"content": content},
                                                    )()
                                                },
                                            )()
                                        ]
                                    },
                                )()
                            else:
                                self.logger.warning(
                                    f"ğŸ“„ Google Gemini content.partsê°€ ì—†ìŒ: {candidate.content}"
                                )
                        else:
                            self.logger.warning(
                                f"ğŸ“„ Google Gemini contentê°€ ì—†ìŒ: {candidate}"
                            )
                    else:
                        self.logger.info(
                            f"ğŸ“„ Google Gemini ì‘ë‹µ íƒ€ì…: {type(response)}"
                        )
                elif config.provider == LLMProvider.MCP:
                    if hasattr(response, "choices") and response.choices:
                        content = response.choices[0].message.content
                        output_tokens = count_tokens(content or "", config.model)
                        self.logger.info(f"ğŸ“¤ ì¶œë ¥ í† í° ìˆ˜: {output_tokens}")
                        self.logger.info(f"ğŸ“„ LLM ì‘ë‹µ ë‚´ìš©: {repr(content)}")
                    else:
                        self.logger.info(f"ğŸ“„ LLM ì‘ë‹µ íƒ€ì…: {type(response)}")

                return response

            except asyncio.TimeoutError:
                last_error = f"íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1})"
                self.logger.warning(f"LLM í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ: {last_error}")
            except Exception as e:
                last_error = str(e)
                self.logger.warning(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {last_error}")

            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ëŒ€ê¸°
            if attempt < config.max_retries - 1:
                await asyncio.sleep(config.retry_delay)

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        raise Exception(f"LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {last_error}")

    async def _process_streaming_response(
        self, stream, config: LLMConfig, session_id: int = None, streams_client=None
    ) -> str:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… content ë°˜í™˜ (ì‹¤ì‹œê°„ ì²­í¬ ì „ì†¡ í¬í•¨)"""
        try:
            full_content = ""
            chunk_count = 0

            self.logger.info(f"ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘: provider={config.provider.value}")

            async for chunk in stream:
                try:
                    content = ""

                    # OpenAI í˜•ì‹ (OpenAI, Anthropic, MCP)
                    if (
                        hasattr(chunk, "choices")
                        and chunk.choices
                        and len(chunk.choices) > 0
                    ):
                        delta = chunk.choices[0].delta
                        if hasattr(delta, "content"):
                            content = delta.content or ""

                    # Google Gemini í˜•ì‹
                    elif hasattr(chunk, "candidates") and chunk.candidates:
                        candidate = chunk.candidates[0]
                        if hasattr(candidate, "content") and candidate.content:
                            if (
                                hasattr(candidate.content, "parts")
                                and candidate.content.parts
                            ):
                                content = candidate.content.parts[0].text or ""

                    # ì§ì ‘ content ì†ì„±ì´ ìˆëŠ” ê²½ìš° (ì¼ë¶€ í”„ë¡œë°”ì´ë”)
                    elif hasattr(chunk, "content"):
                        content = chunk.content or ""

                    # contentê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
                    if isinstance(content, list):
                        content = "".join(str(item) for item in content)

                    # contentê°€ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
                    if not isinstance(content, str):
                        content = str(content) if content else ""

                    # ë¹ˆ ì²­í¬ë„ ì¹´ìš´íŠ¸ì— í¬í•¨ (ì—°ì†ì„± ìœ ì§€)
                    chunk_count += 1
                    full_content += content

                    # ì‹¤ì‹œê°„ ì²­í¬ ì „ì†¡ (ì„¸ì…˜ IDê°€ ìˆê³  streams_clientê°€ ìˆëŠ” ê²½ìš°)
                    # contentê°€ ìˆê±°ë‚˜ ë¹ˆ ì²­í¬ë¼ë„ ì „ì†¡ (ì—°ì†ì„± ìœ ì§€)
                    if session_id and streams_client:
                        try:
                            # í†µì¼ëœ streaming_chunk íƒ€ì…ìœ¼ë¡œ ì „ì†¡
                            stream_data = {
                                "type": "streaming_chunk",
                                "session_id": session_id,
                                "content": content,
                                "chunk_index": chunk_count - 1,  # 0ë¶€í„° ì‹œì‘
                                "is_final": False,
                                "timestamp": datetime.now().isoformat(),
                            }

                            # request_idê°€ ìˆìœ¼ë©´ ì¶”ê°€
                            request_id_param = getattr(
                                self, "_current_request_id", None
                            )
                            if request_id_param:
                                stream_data["request_id"] = request_id_param

                            await streams_client.send_to_backend_stream(stream_data)
                            # if content:
                            #     self.logger.info(f"ğŸ“¤ ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì „ì†¡: {chunk_count} - {content[:50]}...")
                            # else:
                            #     self.logger.info(f"ğŸ“¤ ìŠ¤íŠ¸ë¦¬ë° ë¹ˆ ì²­í¬ ì „ì†¡: {chunk_count}")
                        except Exception as e:
                            self.logger.warning(f"ì‹¤ì‹œê°„ ì²­í¬ ì „ì†¡ ì‹¤íŒ¨: {e}")
                    else:
                        self.logger.warning(
                            f"ì²­í¬ ì „ì†¡ ì‹¤íŒ¨: session_id={session_id}, streams_client={streams_client}"
                        )

                except Exception as chunk_error:
                    self.logger.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {chunk_error}")
                    # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ì²­í¬ ì¹´ìš´íŠ¸ëŠ” ì¦ê°€ì‹œì¼œì„œ ì—°ì†ì„± ìœ ì§€
                    chunk_count += 1

            self.logger.info(
                f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì™„ë£Œ: ì´ {chunk_count}ê°œ ì²­í¬, {len(full_content)}ì"
            )
            self.logger.info(
                f"ğŸ” ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ - full_content ëë¶€ë¶„(ë§ˆì§€ë§‰ 100ì): ...{full_content[-100:] if len(full_content) > 100 else full_content}"
            )

            # ìµœì¢… ì²­í¬ ì „ì†¡ (ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹ í˜¸)
            if session_id and streams_client:
                try:
                    # í†µì¼ëœ streaming_complete íƒ€ì…ìœ¼ë¡œ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡
                    await streams_client.send_to_backend_stream(
                        {
                            "type": "streaming_complete",
                            "session_id": session_id,
                            "full_response": full_content,
                            "total_chunks": chunk_count,
                            "is_final": True,
                            "timestamp": datetime.now().isoformat(),
                        }
                    )
                    self.logger.info(
                        f"ğŸ“¤ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹ í˜¸ ì „ì†¡: ì„¸ì…˜ {session_id}, ì´ {chunk_count}ê°œ ì²­í¬, full_response ê¸¸ì´ {len(full_content)}ì"
                    )
                except Exception as e:
                    self.logger.warning(f"ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ ì‹¤íŒ¨: {e}")

            # LLM ì‘ë‹µì—ì„œ text ë¶€ë¶„ë§Œ ì¶”ì¶œ (MCPì™€ ì¼ë°˜ ëª¨ë¸ êµ¬ë¶„)
            try:
                import json
                import ast

                # MCP ì‘ë‹µ í˜•íƒœ: [{'type': 'text', 'text': '...'}]
                if (
                    full_content.strip().startswith("[")
                    and "type" in full_content
                    and "text" in full_content
                ):
                    # MCP ì‘ë‹µ íŒŒì‹±
                    try:
                        response_list = ast.literal_eval(full_content)
                        if isinstance(response_list, list) and len(response_list) > 0:
                            first_item = response_list[0]
                            if isinstance(first_item, dict) and "text" in first_item:
                                full_content = first_item["text"]
                                self.logger.info("MCP ì‘ë‹µì—ì„œ text ë¶€ë¶„ ì¶”ì¶œ ì™„ë£Œ")
                    except (ValueError, SyntaxError) as e:
                        self.logger.warning(f"MCP ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
                # ì¼ë°˜ JSON í˜•íƒœ: {'type': 'text', 'text': '...'}
                elif (
                    full_content.strip().startswith("{")
                    and "type" in full_content
                    and "text" in full_content
                ):
                    response_data = json.loads(full_content)
                    if isinstance(response_data, dict) and "text" in response_data:
                        full_content = response_data["text"]
                        self.logger.info("ì¼ë°˜ JSON ì‘ë‹µì—ì„œ text ë¶€ë¶„ ì¶”ì¶œ ì™„ë£Œ")
                else:
                    self.logger.info("LLM ì‘ë‹µì´ JSON í˜•íƒœê°€ ì•„ë‹˜, ê·¸ëŒ€ë¡œ ì‚¬ìš©")
            except Exception as e:
                self.logger.warning(f"LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")

            return full_content

        except Exception as e:
            self.logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    @staticmethod
    def _create_mcp_response(
        content: str, is_streaming: bool = False, gpt5_streaming: bool = False
    ):
        """MCP ì‘ë‹µì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""

        class MockChoice:
            def __init__(self, content):
                self.message = MockMessage(content)
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ delta ì†ì„± ì¶”ê°€
                self.delta = MockDelta(content)

        class MockMessage:
            def __init__(self, content):
                self.content = content

        class MockDelta:
            def __init__(self, content):
                self.content = content

        class MockResponse:
            def __init__(self, content, is_streaming=False, gpt5_streaming=False):
                self.choices = [MockChoice(content)]
                # ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±ë“¤ ì¶”ê°€
                self.text = content  # ì¼ë¶€ ì½”ë“œì—ì„œ ì‚¬ìš©
                self.content = content  # ì¼ë¶€ ì½”ë“œì—ì„œ ì‚¬ìš©
                # ë¬¸ìì—´ë¡œë„ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡
                self.__str__ = lambda: content
                self.__repr__ = lambda: f"MockResponse('{content[:50]}...')"
                self._iterated = False
                self._is_streaming = is_streaming
                self._gpt5_streaming = gpt5_streaming
                self._content = content
                self._chunk_index = 0

                # ìŠ¤íŠ¸ë¦¬ë° ì •ë³´ ë¡œê¹…
                if is_streaming or gpt5_streaming:
                    print(
                        f"ğŸŒŠ MCP ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ê°ì§€: streaming={is_streaming}, gpt5_streaming={gpt5_streaming}"
                    )
                    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° (í•œ ë²ˆì— 10-20ìì”©)
                    self._chunks = LLMTool._split_into_chunks(content, chunk_size=15)
                    print(f"ğŸ“ í…ìŠ¤íŠ¸ë¥¼ {len(self._chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")

            def __aiter__(self):
                """ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ async iterator"""
                return self

            async def __anext__(self):
                """ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ async next"""
                if not self._is_streaming and not self._gpt5_streaming:
                    # ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹Œ ê²½ìš° í•œ ë²ˆë§Œ ë°˜í™˜
                    if not self._iterated:
                        self._iterated = True
                        return self
                    raise StopAsyncIteration

                # ìŠ¤íŠ¸ë¦¬ë°ì¸ ê²½ìš° ì²­í¬ ë‹¨ìœ„ë¡œ ë°˜í™˜
                if self._chunk_index < len(self._chunks):
                    chunk = self._chunks[self._chunk_index]
                    self._chunk_index += 1

                    # ì²­í¬ ë‚´ìš©ìœ¼ë¡œ MockResponse ìƒì„± (ë¡œê¹… ì—†ì´)
                    chunk_response = MockResponse(
                        chunk, is_streaming=False, gpt5_streaming=False
                    )
                    chunk_response._iterated = True  # ì²­í¬ëŠ” í•œ ë²ˆë§Œ ë°˜í™˜
                    return chunk_response

                raise StopAsyncIteration

        return MockResponse(content, is_streaming, gpt5_streaming)

    @staticmethod
    def _split_into_chunks(text: str, chunk_size: int = 15) -> list:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        chunks = []
        for i in range(0, len(text), chunk_size):
            chunk = text[i : i + chunk_size]
            chunks.append(chunk)
        return chunks

    @staticmethod
    async def _call_mcp_realtime_streaming(
        mcp_url: str, messages: List[Dict[str, str]], config: LLMConfig
    ):
        """MCP ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ"""

        # API ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ë°ì´í„° êµ¬ì„±
        mcp_data = {
            "message": f"System: {messages[0]['content']}\n\nUser: {messages[-1]['content']}"
            if messages and messages[0]["role"] == "system"
            else messages[-1]["content"]
            if messages
            else "",
            "chat_hash": "maice-session",
        }

        logger.debug(f"ğŸŒŠ API ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­: {mcp_url}")
        logger.debug(f"ğŸŒŠ Message: {mcp_data['message'][:100]}...")

        class RealtimeStreamingResponse:
            def __init__(self, mcp_url: str, mcp_data: dict, config: LLMConfig):
                self.mcp_url = mcp_url
                self.mcp_data = mcp_data
                self.config = config
                self._session = None
                self._response = None
                self._is_streaming = True
                self._gpt5_streaming = True
                self._session_id = (
                    f"session_{id(self)}_{datetime.now().timestamp()}"  # ê³ ìœ  ì„¸ì…˜ ID
                )
                self._buffer = ""  # ì²­í¬ ë²„í¼ë§ì„ ìœ„í•œ ë²„í¼
                self._buffer_size = 50  # ìµœì†Œ 50ì ì´ìƒ ëª¨ì¼ ë•Œ ì „ì†¡
                self._completed = False  # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í”Œë˜ê·¸ ì¶”ê°€

            async def __aenter__(self):
                self._client = httpx.AsyncClient(timeout=config.timeout)
                return self

            async def __aexit__(self, exc_type, exc_val, exc_tb):
                if self._client:
                    await self._client.aclose()
                if self._session:
                    await self._session.close()

            def __aiter__(self):
                return self

            async def __anext__(self):
                # ìŠ¤íŠ¸ë¦¬ë°ì´ ì™„ë£Œëœ ê²½ìš° ì¦‰ì‹œ ì¢…ë£Œ
                if self._completed:
                    raise StopAsyncIteration

                if not self._session:
                    # ê° ìš”ì²­ë§ˆë‹¤ ë…ë¦½ì ì¸ aiohttp ì„¸ì…˜ ìƒì„±
                    timeout = aiohttp.ClientTimeout(total=self.config.timeout)
                    self._session = aiohttp.ClientSession(
                        timeout=timeout,
                        connector=aiohttp.TCPConnector(
                            limit=100, limit_per_host=30
                        ),  # ì—°ê²° í’€ ì„¤ì •
                    )
                    logger.info(f"ğŸ”— ìƒˆë¡œìš´ aiohttp ì„¸ì…˜ ìƒì„±: {self._session_id}")

                if not self._response:
                    # aiohttpë¡œ ì‹¤ì‹œê°„ SSE ìš”ì²­
                    self._response = await self._session.post(
                        self.mcp_url,
                        json=self.mcp_data,
                        headers={"Content-Type": "application/json"},
                    )
                    self._response.raise_for_status()

                # aiohttpë¡œ ì‹¤ì‹œê°„ ë¼ì¸ ì½ê¸°
                try:
                    line_bytes = await self._response.content.readline()
                    if not line_bytes:
                        # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
                        logger.info(f"âœ… ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ({self._session_id})")
                        self._completed = True
                        if self._session:
                            await self._session.close()
                        raise StopAsyncIteration

                    line = line_bytes.decode("utf-8").strip()
                    if line:  # ë¹ˆ ë¼ì¸ ì²´í¬ ì¶”ê°€
                        logger.info(f"ğŸ” aiohttp SSE ë¼ì¸: {line[:50]}...")

                    if line and line.startswith("data: "):
                        data_str = line[6:]  # "data: " ì œê±°

                        try:
                            data = json.loads(data_str)
                            logger.info(f"ğŸ” aiohttp SSE ë°ì´í„°: {data}")

                            if data.get("type") == "delta":
                                # ë¸íƒ€ ì²­í¬ ìƒì„± í›„ ì¦‰ì‹œ ë°˜í™˜
                                text_content = data.get("text", "")

                                # JSON ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš° í•„í„°ë§
                                if text_content and (
                                    text_content.startswith(
                                        '{"name":"message_output_created"'
                                    )
                                    or text_content.startswith(
                                        '{"type":"stream_complete"'
                                    )
                                    or text_content.startswith(
                                        '{"type":"run_item_stream_event"'
                                    )
                                    or "message_output_created" in text_content
                                    or "run_item_stream_event" in text_content
                                ):
                                    logger.debug(
                                        f"ğŸ” JSON ë©”íƒ€ë°ì´í„° í•„í„°ë§: {text_content[:50]}..."
                                    )
                                    # JSON ë©”íƒ€ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸°
                                    return await self.__anext__()

                                # ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬ (ë²„í¼ë§ ì ìš©) - ì¤„ë°”ê¿ˆë„ í¬í•¨
                                if text_content is not None and text_content != "":
                                    # ë²„í¼ì— í…ìŠ¤íŠ¸ ì¶”ê°€
                                    self._buffer += text_content

                                    # ë²„í¼ê°€ ì¶©ë¶„íˆ ìŒ“ì´ê±°ë‚˜ ë¬¸ì¥ì´ ëë‚˜ëŠ” ê²½ìš° ì „ì†¡
                                    if len(
                                        self._buffer
                                    ) >= self._buffer_size or self._buffer.endswith(
                                        (".", "!", "?", "ã€‚", "!", "?", "\n")
                                    ):
                                        logger.info(
                                            f"ğŸ“¤ aiohttp ë²„í¼ë§ëœ ì²­í¬ ì „ì†¡: '{self._buffer}' (ê¸¸ì´: {len(self._buffer)})"
                                        )
                                        chunk_response = LLMTool._create_mcp_response(
                                            self._buffer,
                                            is_streaming=True,
                                            gpt5_streaming=True,
                                        )
                                        self._buffer = ""  # ë²„í¼ ì´ˆê¸°í™”
                                        return chunk_response
                                    else:
                                        # ì•„ì§ ì „ì†¡í•˜ì§€ ì•Šê³  ë‹¤ìŒ ì²­í¬ ëŒ€ê¸°
                                        return await self.__anext__()
                                else:
                                    # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ê±´ë„ˆë›°ê¸°
                                    return await self.__anext__()

                            elif data.get("type") == "completed":
                                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ - ë‚¨ì€ ë²„í¼ ì „ì†¡
                                if self._buffer:
                                    logger.info(
                                        f"ğŸ“¤ aiohttp ìµœì¢… ë²„í¼ ì „ì†¡: '{self._buffer}' (ê¸¸ì´: {len(self._buffer)})"
                                    )
                                    chunk_response = LLMTool._create_mcp_response(
                                        self._buffer,
                                        is_streaming=True,
                                        gpt5_streaming=True,
                                    )
                                    self._buffer = ""
                                    return chunk_response

                                logger.info(
                                    f"âœ… aiohttp ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ({self._session_id})"
                                )
                                self._completed = True  # ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
                                if self._session:
                                    await self._session.close()
                                raise StopAsyncIteration

                        except json.JSONDecodeError:
                            pass

                    # deltaê°€ ì•„ë‹Œ ë¼ì¸ì€ ë‹¤ìŒ ë¼ì¸ ìš”ì²­
                    return await self.__anext__()

                except StopAsyncIteration:
                    # ì •ìƒì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
                    self._completed = True
                    if self._session:
                        await self._session.close()
                    raise
                except Exception as e:
                    error_msg = str(e) if e else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                    logger.error(f"aiohttp SSE ì˜¤ë¥˜ ({self._session_id}): {error_msg}")
                    self._completed = True  # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì™„ë£Œë¡œ ì²˜ë¦¬
                    if self._session:
                        await self._session.close()
                    raise StopAsyncIteration

        return RealtimeStreamingResponse(mcp_url, mcp_data, config)

    # ì‹œë®¬ë ˆì´ì…˜ëœ ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ì œê±°ë¨ - ì´ì œ ì‹¤ì œ MCP ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©


class SpecializedLLMTool(LLMTool):
    """íŠ¹í™”ëœ LLM íˆ´ë“¤"""

    @staticmethod
    def _get_provider_from_env():
        """í™˜ê²½ë³€ìˆ˜ì—ì„œ LLM_PROVIDER ì½ê¸°"""
        env_provider = os.getenv("LLM_PROVIDER", "openai").lower()
        if env_provider == "anthropic":
            return LLMProvider.ANTHROPIC
        elif env_provider == "google":
            return LLMProvider.GOOGLE
        elif env_provider == "mcp":
            return LLMProvider.MCP
        else:
            return LLMProvider.OPENAI

    @classmethod
    def create_classifier_tool(cls) -> "SpecializedLLMTool":
        """ì§ˆë¬¸ ë¶„ë¥˜ìš© LLM íˆ´"""
        config = LLMConfig(
            provider=cls._get_provider_from_env(),
            max_tokens=5000,  # 3000 â†’ 5000ìœ¼ë¡œ ì¦ê°€ (JSON ì‘ë‹µ ì™„ì„± ë³´ì¥)
            timeout=300,  # 120 â†’ 300ì´ˆë¡œ ì¦ê°€ (ì‘ë‹µ ì™„ì„± ë³´ì¥)
            max_retries=5,  # 3 â†’ 5ë¡œ ì¦ê°€
            stream=False,  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™” (JSON ì‘ë‹µì„ ìœ„í•´)
        )

        template = PromptTemplate(
            system_prompt="ë‹¹ì‹ ì€ ìˆ˜í•™ ì§ˆë¬¸ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
            user_template="ì§ˆë¬¸ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”: {question}",
        )

        return cls("classifier_llm", config, template)

    @classmethod
    def create_answer_generator_tool(cls) -> "SpecializedLLMTool":
        """ë‹µë³€ ìƒì„±ìš© LLM íˆ´"""
        config = LLMConfig(
            provider=cls._get_provider_from_env(),
            max_tokens=4000,  # í•œê¸€ì€ í† í°ì´ ë§ì´ í•„ìš”í•˜ë¯€ë¡œ ì¶©ë¶„í•˜ê²Œ ì„¤ì •
            timeout=60,  # Anthropicì€ ëŠë¦¬ë¯€ë¡œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
            max_retries=3,
            stream=True,
        )

        template = PromptTemplate(
            system_prompt="ë‹¹ì‹ ì€ ìˆ˜í•™ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
            user_template="í•™ìƒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”: {question}",
        )

        return cls("answer_generator_llm", config, template)

    @classmethod
    def create_observer_tool(cls) -> "SpecializedLLMTool":
        """í•™ìŠµ ê´€ì°° ë° ìš”ì•½ìš© LLM íˆ´"""
        config = LLMConfig(
            provider=cls._get_provider_from_env(),
            max_tokens=1000,  # ìš”ì•½ìš©ìœ¼ë¡œ ì ë‹¹í•œ í¬ê¸°
            timeout=60,
            max_retries=2,
            stream=False,  # JSON ì‘ë‹µì„ ìœ„í•´ ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”
        )

        template = PromptTemplate(
            system_prompt="ë‹¹ì‹ ì€ í•™ìŠµ ê³¼ì • ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
            user_template="í•™ìŠµ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”: {conversation}",
        )

        return cls("observer_llm", config, template)

    @classmethod
    def create_freetalker_tool(cls) -> "SpecializedLLMTool":
        """í”„ë¦¬í† ì»¤ ì—ì´ì „íŠ¸ìš© LLM íˆ´"""
        config = LLMConfig(
            provider=cls._get_provider_from_env(),
            max_tokens=4000,  # í”„ë¦¬íŒ¨ìŠ¤ ëª¨ë“œìš© ì¶©ë¶„í•œ í† í°
            timeout=60,  # ì ë‹¹í•œ íƒ€ì„ì•„ì›ƒ
            max_retries=3,
            stream=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        )

        template = PromptTemplate(
            system_prompt="í•„ìš”í•  ë•Œë§Œ ìˆ˜í•™ ìˆ˜ì‹ì„ LaTeX í˜•ì‹($ìˆ˜ì‹$)ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            user_template="{question}",
        )

        return cls("freetalker_llm", config, template)

    @classmethod
    def create_question_improvement_tool(cls) -> "SpecializedLLMTool":
        """ì§ˆë¬¸ ê°œì„ ìš© LLM íˆ´"""
        config = LLMConfig(
            provider=cls._get_provider_from_env(),
            max_tokens=3000,  # 800 â†’ 2000ìœ¼ë¡œ ì¦ê°€ (ì…ë ¥ í† í° 2631ê°œ + ì‘ë‹µ í† í° ì—¬ìœ ë¶„)
            timeout=60,  # 30 â†’ 60ì´ˆë¡œ ì¦ê°€ (gpt-5-miniëŠ” ì‘ë‹µì´ ëŠë¦¼)
            max_retries=2,
        )

        template = PromptTemplate(
            system_prompt="ë‹¹ì‹ ì€ ì§ˆë¬¸ ê°œì„  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
            user_template="ì§ˆë¬¸ì„ ê°œì„ í•´ì£¼ì„¸ìš”: {question}",
        )

        return cls("question_improvement_llm", config, template)

#!/usr/bin/env python3
"""
LLM ê¸°ë°˜ ë”¥í˜ì´í¬ì‹ í•™ìƒ ì§ˆë¬¸ ìƒì„±ê¸°
ì‹¤ì œ í•™ìƒë“¤ì´ GPTì— ì§ˆë¬¸í•œ í˜•íƒœë¥¼ LLMì´ í•™ìŠµí•´ì„œ ë¹„ìŠ·í•œ ë¬¸ì²´ì™€ íŒ¨í„´ìœ¼ë¡œ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±
"""

import json
import os
import random
import re
import asyncio
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import logging
from datetime import datetime

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install python-dotenvë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    print("í™˜ê²½ë³€ìˆ˜ë¥¼ ì§ì ‘ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# OpenAI í´ë¼ì´ì–¸íŠ¸
try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logging.warning("openaië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install openai'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# ë¡œê¹… ì„¤ì •
log_level = os.getenv('LOG_LEVEL', 'INFO')
logging.basicConfig(
    level=getattr(logging, log_level.upper()),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm_deepfake_generator.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class LLMDeepfakeGenerator:
    """LLMì„ í™œìš©í•œ ë”¥í˜ì´í¬ì‹ í•™ìƒ ì§ˆë¬¸ ìƒì„±ê¸°"""
    
    def __init__(self, openai_api_key: str = None, model: str = None, data_path: str = None):
        self.openai_client = None
        self.model = model or os.getenv('OPENAI_MODEL', 'gpt-4')
        self.data_path = data_path or os.getenv('DATA_PATH', 'data/evaluation_statistics.json')
        self.real_questions = []
        self.student_profiles = []
        self.style_analysis = {}
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if OPENAI_AVAILABLE:
            api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
            if api_key:
                self.openai_client = AsyncOpenAI(api_key=api_key)
                logging.info(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")
            else:
                logging.warning("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                logging.warning("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        else:
            logging.warning("âŒ OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        self.load_real_data()
        self.analyze_student_styles()
    
    def load_real_data(self):
        """ì‹¤ì œ í•™ìƒ ì§ˆë¬¸ ë°ì´í„° ë¡œë“œ"""
        try:
            # .envì—ì„œ ì„¤ì •ëœ ê²½ë¡œ ìš°ì„  ì‚¬ìš©
            possible_paths = [
                self.data_path,
                'data/evaluation_statistics.json',
                'tester/data/evaluation_statistics.json',
                '../data/evaluation_statistics.json'
            ]
            
            data = None
            for path in possible_paths:
                if os.path.exists(path):
                    with open(path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    logging.info(f"âœ… ë°ì´í„° íŒŒì¼ ë¡œë“œ ì„±ê³µ: {path}")
                    break
            
            if not data:
                logging.warning("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                logging.warning(f"ì‹œë„í•œ ê²½ë¡œë“¤: {possible_paths}")
                return
            
            # ì§ˆë¬¸ë³„_êµì‚¬_í‰ê°€ì—ì„œ ì§ˆë¬¸ ì¶”ì¶œ
            if 'ì§ˆë¬¸ë³„_êµì‚¬_í‰ê°€' in data:
                max_questions = int(os.getenv('MAX_QUESTIONS', 1000))
                question_count = 0
                
                for question_id, question_data in data['ì§ˆë¬¸ë³„_êµì‚¬_í‰ê°€'].items():
                    if question_count >= max_questions:
                        break
                        
                    if isinstance(question_data, dict) and 'ì§ˆë¬¸_ì›ë¬¸' in question_data:
                        question_text = question_data['ì§ˆë¬¸_ì›ë¬¸']
                        if isinstance(question_text, str) and 5 <= len(question_text) <= 500:
                            self.real_questions.append(question_text)
                            question_count += 1
            
            logging.info(f"ë¡œë“œëœ ì‹¤ì œ ì§ˆë¬¸ ìˆ˜: {len(self.real_questions)} (ìµœëŒ€: {max_questions})")
            
        except Exception as e:
            logging.error(f"ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            logging.error("ë°ì´í„° íŒŒì¼ ê²½ë¡œì™€ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”")
    
    def analyze_student_styles(self):
        """í•™ìƒë“¤ì˜ ë¬¸ì²´ ìŠ¤íƒ€ì¼ ë¶„ì„"""
        if not self.real_questions:
            return
        
        # ë¬¸ì²´ íŒ¨í„´ ë¶„ì„
        for question in self.real_questions:
            style_info = self._extract_style_features(question)
            self.style_analysis[question] = style_info
        
        logging.info(f"ìŠ¤íƒ€ì¼ ë¶„ì„ ì™„ë£Œ: {len(self.style_analysis)}ê°œ ì§ˆë¬¸")
    
    def _extract_style_features(self, question: str) -> Dict[str, any]:
        """ì§ˆë¬¸ì—ì„œ ìŠ¤íƒ€ì¼ íŠ¹ì§• ì¶”ì¶œ"""
        features = {
            'formality': self._analyze_formality(question),
            'emotion': self._analyze_emotion(question),
            'urgency': self._analyze_urgency(question),
            'uncertainty': self._analyze_uncertainty(question),
            'length': len(question),
            'has_emoji': bool(re.search(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿]', question)),
            'has_ellipsis': bool(re.search(r'\.{2,}', question)),
            'has_exclamation': bool(re.search(r'[ì•„|ì–´|ì˜¤|ìš°|ìœ¼|ì´]+\!+', question)),
            'sentence_endings': self._analyze_sentence_endings(question),
            'vocabulary_level': self._analyze_vocabulary_level(question)
        }
        return features
    
    def _analyze_formality(self, text: str) -> str:
        """ì¡´ëŒ“ë§/ë°˜ë§ ë¶„ì„"""
        formal_count = len(re.findall(r'[ìš”|ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|ë‹ˆë‹¤]', text))
        informal_count = len(re.findall(r'[ì•¼|ì–´|ì•„|ì§€]', text))
        
        if formal_count > informal_count:
            return 'formal'
        elif informal_count > formal_count:
            return 'informal'
        else:
            return 'mixed'
    
    def _analyze_emotion(self, text: str) -> str:
        """ê°ì • ìƒíƒœ ë¶„ì„"""
        emotion_keywords = {
            'frustrated': ['ì§œì¦', 'ë‹µë‹µ', 'í™”ë‚˜', 'ì—´ë°›', 'ìŠ¤íŠ¸ë ˆìŠ¤'],
            'anxious': ['ë¶ˆì•ˆ', 'ê±±ì •', 'ë‘ë ¤', 'ê¸´ì¥', 'ë–¨ë ¤'],
            'confused': ['í—·ê°ˆë ¤', 'ëª¨ë¥´ê² ì–´', 'ì–´ë ¤ì›Œ', 'ë³µì¡í•´'],
            'excited': ['ì¬ë°Œì–´', 'ì‹ ë‚˜', 'í¥ë¯¸ë¡œì›Œ', 'ê¶ê¸ˆí•´'],
            'desperate': ['ê¸‰í•´', 'ë¹¨ë¦¬', 'ë‹¹ì¥', 'ë°”ë¡œ', 'ì‹œí—˜'],
            'neutral': []
        }
        
        for emotion, keywords in emotion_keywords.items():
            if any(keyword in text for keyword in keywords):
                return emotion
        
        return 'neutral'
    
    def _analyze_urgency(self, text: str) -> int:
        """ê¸´ê¸‰í•¨ ìˆ˜ì¤€ ë¶„ì„ (0-5)"""
        urgency_words = ['ê¸‰í•´', 'ë¹¨ë¦¬', 'ì‹œí—˜', 'ë‚´ì¼', 'ì˜¤ëŠ˜', 'ë‹¹ì¥', 'ë°”ë¡œ', 'ë§ˆê°']
        urgency_score = sum(1 for word in urgency_words if word in text)
        return min(urgency_score, 5)
    
    def _analyze_uncertainty(self, text: str) -> int:
        """ë¶ˆí™•ì‹¤í•¨ ìˆ˜ì¤€ ë¶„ì„ (0-5)"""
        uncertainty_words = ['ê°™ì€ë°', 'ê°™ì€', 'ê²ƒ ê°™ì€', 'ëª¨ë¥´ê² ì–´', 'ì˜ ëª¨ë¥´ê² ì–´', 'ì•„ë§ˆ', 'í˜¹ì‹œ', 'ì•„ì§']
        uncertainty_score = sum(1 for word in uncertainty_words if word in text)
        return min(uncertainty_score, 5)
    
    def _analyze_sentence_endings(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë í‘œí˜„ ë¶„ì„"""
        endings = []
        if '?' in text:
            endings.append('question')
        if re.search(r'[ìš”|ë‹ˆë‹¤|ìŠµë‹ˆë‹¤]', text):
            endings.append('formal')
        if re.search(r'[ì•¼|ì–´|ì•„|ì§€]', text):
            endings.append('informal')
        if re.search(r'\.{2,}', text):
            endings.append('ellipsis')
        if re.search(r'\!+', text):
            endings.append('exclamation')
        
        return endings
    
    def _analyze_vocabulary_level(self, text: str) -> str:
        """ì–´íœ˜ ìˆ˜ì¤€ ë¶„ì„"""
        advanced_terms = ['ìˆ˜í•™ì ê·€ë‚©ë²•', 'ì í™”ì‹', 'ì¼ë°˜í•­', 'ê³µì°¨', 'ê³µë¹„', 'ì‹œê·¸ë§ˆ', 'Î£']
        intermediate_terms = ['ìˆ˜ì—´', 'ë“±ì°¨ìˆ˜ì—´', 'ë“±ë¹„ìˆ˜ì—´', 'í•©ê³„', 'í•­']
        basic_terms = ['ë”í•˜ê¸°', 'ë¹¼ê¸°', 'ê³±í•˜ê¸°', 'ë‚˜ëˆ„ê¸°', 'ê³„ì‚°']
        
        advanced_count = sum(1 for term in advanced_terms if term in text)
        intermediate_count = sum(1 for term in intermediate_terms if term in text)
        basic_count = sum(1 for term in basic_terms if term in text)
        
        if advanced_count > 0:
            return 'advanced'
        elif intermediate_count > 0:
            return 'intermediate'
        elif basic_count > 0:
            return 'basic'
        else:
            return 'unknown'
    
    async def generate_llm_deepfake_question(self, 
                                           target_topic: str,
                                           style_profile: str = None,
                                           difficulty: str = 'intermediate',
                                           emotion: str = None) -> str:
        """LLMì„ í™œìš©í•œ ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„±"""
        if not self.openai_client:
            return "OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        
        # ìŠ¤íƒ€ì¼ í”„ë¡œí•„ ì„ íƒ
        if style_profile and style_profile in self.style_analysis:
            selected_style = style_profile
        else:
            # ëœë¤í•˜ê²Œ ìŠ¤íƒ€ì¼ ì„ íƒ
            available_styles = list(self.style_analysis.keys())
            if available_styles:
                selected_style = random.choice(available_styles)
            else:
                return "ë¶„ì„ëœ ìŠ¤íƒ€ì¼ì´ ì—†ìŠµë‹ˆë‹¤."
        
        style_features = self.style_analysis[selected_style]
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_llm_prompt(target_topic, style_features, difficulty, emotion)
        
        # ë””ë²„ê¹…: í”„ë¡¬í”„íŠ¸ ë‚´ìš© ë¡œê¹…
        logging.info(f"ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}")
        logging.info(f"í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {prompt[:200]}...")
        
        try:
            # GPT-5-mini ëª¨ë¸ì€ max_completion_tokensë¥¼ ì‚¬ìš©í•˜ê³  temperatureëŠ” ê¸°ë³¸ê°’ë§Œ ì§€ì›
            if 'gpt-5' in self.model:
                logging.info(f"GPT-5 ëª¨ë¸ ì‚¬ìš©: {self.model}")
                
                # GPT-5-mini ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì •
                request_params = {
                    "model": self.model,
                    "messages": [
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™ìƒë“¤ì˜ ìˆ˜í•™ ì§ˆë¬¸ ìŠ¤íƒ€ì¼ì„ ì •í™•í•˜ê²Œ ëª¨ë°©í•˜ëŠ” AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìŠ¤íƒ€ì¼ ê°€ì´ë“œì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê³  í˜„ì‹¤ì ì¸ í•™ìƒ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_completion_tokens": 1000  # ë” í° ê°’ìœ¼ë¡œ ì¡°ì •
                }
                
                logging.info(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: {request_params}")
                
                response = await self.openai_client.chat.completions.create(**request_params)
                
                # ë””ë²„ê¹…: ì „ì²´ ì‘ë‹µ êµ¬ì¡° í™•ì¸
                logging.info(f"ì‘ë‹µ êµ¬ì¡°: {response}")
                logging.info(f"ì‘ë‹µ choices: {response.choices}")
                
            else:
                # ê¸°ì¡´ ëª¨ë¸ë“¤ì€ max_tokensì™€ temperature ì‚¬ìš©
                logging.info(f"ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©: {self.model}")
                response = await self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™ìƒë“¤ì˜ ìˆ˜í•™ ì§ˆë¬¸ ìŠ¤íƒ€ì¼ì„ ì •í™•í•˜ê²Œ ëª¨ë°©í•˜ëŠ” AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìŠ¤íƒ€ì¼ ê°€ì´ë“œì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê³  í˜„ì‹¤ì ì¸ í•™ìƒ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=0.8,
                    max_tokens=200
                )
            
            # ë””ë²„ê¹…: API ì‘ë‹µ ë¡œê¹…
            if response.choices and len(response.choices) > 0:
                content = response.choices[0].message.content
                logging.info(f"API ì‘ë‹µ ì„±ê³µ: '{content}' (ê¸¸ì´: {len(content) if content else 0})")
                
                if content and content.strip():
                    generated_question = content.strip()
                    
                    # ìƒì„±ëœ ì§ˆë¬¸ì˜ í’ˆì§ˆ ê²€ì¦
                    if self._validate_generated_question(generated_question, style_features):
                        logging.info(f"LLM ìƒì„± ì„±ê³µ: {generated_question}")
                        return generated_question
                    else:
                        logging.warning(f"LLM ìƒì„± ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {generated_question}")
                        # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„±
                        return self._fallback_generation(target_topic, style_features, difficulty)
                else:
                    logging.error("API ì‘ë‹µ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")
                    return self._fallback_generation(target_topic, style_features, difficulty)
            else:
                logging.error("API ì‘ë‹µì— choicesê°€ ì—†ìŒ")
                return self._fallback_generation(target_topic, style_features, difficulty)
                
        except Exception as e:
            logging.error(f"LLM ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._fallback_generation(target_topic, style_features, difficulty)
    
    def _create_llm_prompt(self, target_topic: str, style_features: Dict, difficulty: str, emotion: str = None) -> str:
        """LLM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ì‹¤ì œ í•™ìƒ ì§ˆë¬¸ ì˜ˆì‹œë“¤ì„ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ì„ íƒ
        style_examples = self._get_style_examples(style_features)
        
        # ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ìƒì„±
        style_guide = self._generate_style_guide(style_features)
        
        # ë‚œì´ë„ ê°€ì´ë“œ
        difficulty_guide = {
            'naive': 'ê¸°ë³¸ ê°œë…ì— ëŒ€í•œ ê°„ë‹¨í•œ ì§ˆë¬¸',
            'basic': 'ê¸°ì´ˆì ì¸ ì´í•´ë¥¼ ìœ„í•œ ì§ˆë¬¸',
            'intermediate': 'ì¼ë°˜ì ì¸ ìˆ˜ì¤€ì˜ ì§ˆë¬¸',
            'advanced': 'ì‹¬í™” ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸',
            'olympiad': 'ì˜¬ë¦¼í”¼ì•„ë“œ ìˆ˜ì¤€ì˜ ë„ì „ì  ì§ˆë¬¸'
        }
        
        # ê°ì • ê°€ì´ë“œ
        emotion_guide = {
            'frustrated': 'ë‹µë‹µí•˜ê³  ì§œì¦ë‚˜ëŠ” í†¤',
            'anxious': 'ë¶ˆì•ˆí•˜ê³  ê¸´ì¥ëœ í†¤',
            'confused': 'í—·ê°ˆë¦¬ê³  ì–´ë ¤ì›Œí•˜ëŠ” í†¤',
            'excited': 'í¥ë¯¸ë¡­ê³  ê¶ê¸ˆí•´í•˜ëŠ” í†¤',
            'desperate': 'ê¸‰í•˜ê³  ì ˆë°•í•œ í†¤',
            'neutral': 'í‰ì˜¨í•˜ê³  ì°¨ë¶„í•œ í†¤'
        }
        
        prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™ìƒë“¤ì˜ ìˆ˜í•™ ì§ˆë¬¸ ìŠ¤íƒ€ì¼ì„ ì •í™•í•˜ê²Œ ëª¨ë°©í•˜ëŠ” AIì…ë‹ˆë‹¤.

**ì£¼ì œ**: {target_topic}
**ë‚œì´ë„**: {difficulty_guide.get(difficulty, 'ì¼ë°˜ì ì¸ ìˆ˜ì¤€')}
**ê°ì •**: {emotion_guide.get(emotion or style_features.get('emotion', 'neutral'), 'ìì—°ìŠ¤ëŸ¬ìš´ í†¤')}

**í•™ìŠµí•  í•™ìƒ ì§ˆë¬¸ ì˜ˆì‹œë“¤** (ì´ ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¼ì•¼ í•¨):
{style_examples}

**ìŠ¤íƒ€ì¼ ê°€ì´ë“œ**:
{style_guide}

**ìš”êµ¬ì‚¬í•­**:
1. ìœ„ í•™ìƒ ì§ˆë¬¸ ì˜ˆì‹œë“¤ì˜ ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤
2. ìì—°ìŠ¤ëŸ½ê³  í˜„ì‹¤ì ì¸ í•™ìƒ ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤
3. ì£¼ì œì™€ ë‚œì´ë„ì— ë§ëŠ” ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤
4. í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤
5. ì§ˆë¬¸ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚´ì•¼ í•©ë‹ˆë‹¤

ìœ„ ì˜ˆì‹œë“¤ê³¼ ê°™ì€ í†¤ê³¼ í‘œí˜„ìœ¼ë¡œ '{target_topic}'ì— ëŒ€í•œ í•™ìƒ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
"""
        return prompt
    
    def _get_style_examples(self, style_features: Dict) -> str:
        """ìŠ¤íƒ€ì¼ì— ë§ëŠ” ì‹¤ì œ í•™ìƒ ì§ˆë¬¸ ì˜ˆì‹œë“¤ ê°€ì ¸ì˜¤ê¸°"""
        examples = []
        
        logging.info(f"ìŠ¤íƒ€ì¼ íŠ¹ì§•: {style_features}")
        logging.info(f"ë¶„ì„ëœ ì§ˆë¬¸ ìˆ˜: {len(self.style_analysis)}")
        
        # 20ê°œì˜ ë¬´ì‘ìœ„ í•™ìƒ ì§ˆë¬¸ ì„ íƒ
        available_questions = list(self.style_analysis.keys())
        random_questions = random.sample(available_questions, min(20, len(available_questions)))
        
        for q in random_questions:
            examples.append(f"- {q}")
            logging.info(f"ë¬´ì‘ìœ„ ì„ íƒëœ ì§ˆë¬¸: {q[:50]}...")
        
        result = "\n".join(examples)
        logging.info(f"ìµœì¢… ì„ íƒëœ ì˜ˆì‹œë“¤ ({len(examples)}ê°œ): {result[:200]}...")
        
        return result
    
    def _generate_style_guide(self, style_features: Dict) -> str:
        """ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ìƒì„±"""
        guide_parts = []
        
        # ì¡´ëŒ“ë§/ë°˜ë§ ê°€ì´ë“œ
        formality = style_features.get('formality', 'mixed')
        if formality == 'formal':
            guide_parts.append("- ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš” (ìš”, ë‹ˆë‹¤, ìŠµë‹ˆë‹¤)")
        elif formality == 'informal':
            guide_parts.append("- ë°˜ë§ì„ ì‚¬ìš©í•˜ì„¸ìš” (ì•¼, ì–´, ì•„, ì§€)")
        else:
            guide_parts.append("- ì¡´ëŒ“ë§ê³¼ ë°˜ë§ì„ í˜¼ìš©í•˜ì„¸ìš”")
        
        # ê°ì • ê°€ì´ë“œ
        emotion = style_features.get('emotion', 'neutral')
        if emotion == 'frustrated':
            guide_parts.append("- ë‹µë‹µí•˜ê³  ì§œì¦ë‚˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”")
        elif emotion == 'anxious':
            guide_parts.append("- ë¶ˆì•ˆí•˜ê³  ê¸´ì¥ëœ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”")
        elif emotion == 'confused':
            guide_parts.append("- í—·ê°ˆë¦¬ê³  ì–´ë ¤ì›Œí•˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”")
        elif emotion == 'excited':
            guide_parts.append("- í¥ë¯¸ë¡­ê³  ê¶ê¸ˆí•´í•˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”")
        elif emotion == 'desperate':
            guide_parts.append("- ê¸‰í•˜ê³  ì ˆë°•í•œ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”")
        
        # ê¸´ê¸‰í•¨ ê°€ì´ë“œ
        urgency = style_features.get('urgency', 0)
        if urgency >= 3:
            guide_parts.append("- ê¸´ê¸‰í•¨ì„ í‘œí˜„í•˜ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (ê¸‰í•´, ë¹¨ë¦¬, ì‹œí—˜ ë“±)")
        
        # ë¶ˆí™•ì‹¤í•¨ ê°€ì´ë“œ
        uncertainty = style_features.get('uncertainty', 0)
        if uncertainty >= 2:
            guide_parts.append("- ë¶ˆí™•ì‹¤í•¨ì„ í‘œí˜„í•˜ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (ê°™ì€ë°, ê²ƒ ê°™ì€, ëª¨ë¥´ê² ì–´ ë“±)")
        
        # ì–´íœ˜ ìˆ˜ì¤€ ê°€ì´ë“œ
        vocab_level = style_features.get('vocabulary_level', 'unknown')
        if vocab_level == 'basic':
            guide_parts.append("- ê¸°ë³¸ì ì¸ ìˆ˜í•™ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        elif vocab_level == 'intermediate':
            guide_parts.append("- ì¤‘ê¸‰ ìˆ˜í•™ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        elif vocab_level == 'advanced':
            guide_parts.append("- ê³ ê¸‰ ìˆ˜í•™ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        
        # íŠ¹ìˆ˜ í‘œí˜„ ê°€ì´ë“œ
        if style_features.get('has_emoji', False):
            guide_parts.append("- ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        if style_features.get('has_ellipsis', False):
            guide_parts.append("- ë§ì¤„ì„í‘œ(...)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        if style_features.get('has_exclamation', False):
            guide_parts.append("- ê°íƒ„ì‚¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        
        return "\n".join(guide_parts)
    
    def _validate_generated_question(self, question: str, target_style: Dict) -> bool:
        """ìƒì„±ëœ ì§ˆë¬¸ì˜ í’ˆì§ˆ ê²€ì¦"""
        if not question or len(question) < 5:
            return False
        
        # ê¸°ë³¸ì ì¸ í•œêµ­ì–´ ê²€ì¦
        if not re.search(r'[ê°€-í£]', question):
            return False
        
        # ìŠ¤íƒ€ì¼ ì¼ì¹˜ë„ ê²€ì¦
        generated_features = self._extract_style_features(question)
        
        # ì£¼ìš” ìŠ¤íƒ€ì¼ íŠ¹ì§•ë“¤ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        key_features = ['formality', 'emotion', 'urgency', 'uncertainty']
        match_score = 0
        
        for feature in key_features:
            if feature in target_style and feature in generated_features:
                if target_style[feature] == generated_features[feature]:
                    match_score += 1
        
        # 50% ì´ìƒ ì¼ì¹˜í•˜ë©´ í†µê³¼
        return match_score >= len(key_features) * 0.5
    
    def _fallback_generation(self, target_topic: str, style_features: Dict, difficulty: str) -> str:
        """LLM ìƒì„± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„±"""
        # ê¸°ë³¸ í…œí”Œë¦¿
        templates = {
            'ìˆ˜ì—´': [
                "ë“±ì°¨ìˆ˜ì—´ì˜ ì¼ë°˜í•­ì„ êµ¬í•˜ëŠ” ë°©ë²•ì´ ë­”ê°€ìš”?",
                "ë“±ë¹„ìˆ˜ì—´ì˜ ê³µë¹„ë¥¼ ì–´ë–»ê²Œ êµ¬í•˜ë‚˜ìš”?",
                "ìˆ˜ì—´ì˜ í•©ì„ ê³„ì‚°í•˜ëŠ” ê³µì‹ì´ ê¶ê¸ˆí•´ìš”"
            ],
            'ì í™”ì‹': [
                "ì í™”ì‹ì„ í‘¸ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì¬ê·€ì ìœ¼ë¡œ ì •ì˜ëœ ìˆ˜ì—´ì„ ì–´ë–»ê²Œ í’€ì–´ìš”?",
                "ì í™”ì‹ì˜ ì¼ë°˜í•­ì„ êµ¬í•˜ëŠ” ê³¼ì •ì´ í—·ê°ˆë ¤ìš”"
            ],
            'ê·€ë‚©ë²•': [
                "ìˆ˜í•™ì  ê·€ë‚©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ë­”ê°€ìš”?",
                "ê·€ë‚©ë²•ìœ¼ë¡œ ì¦ëª…í•  ë•Œ n=k+1 ë‹¨ê³„ê°€ ì–´ë ¤ì›Œìš”",
                "ê·€ë‚©ê°€ì •ì„ ì–´ë–»ê²Œ ì„¤ì •í•´ì•¼ í•˜ë‚˜ìš”?"
            ],
            'ìˆ˜ì—´ì˜í•©': [
                "ë“±ì°¨ìˆ˜ì—´ì˜ í•©ì„ êµ¬í•˜ëŠ” ê³µì‹ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ì‹œê·¸ë§ˆë¥¼ ì‚¬ìš©í•´ì„œ ìˆ˜ì—´ì˜ í•©ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ ê¶ê¸ˆí•´ìš”",
                "ìˆ˜ì—´ì˜ í•©ì„ êµ¬í•  ë•Œ ì£¼ì˜í•  ì ì´ ë­”ê°€ìš”?"
            ]
        }
        
        base_templates = templates.get(target_topic, ["ì´ ì£¼ì œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆì–´ìš”"])
        base_question = random.choice(base_templates)
        
        # ìŠ¤íƒ€ì¼ì— ë§ê²Œ ì¡°ì •
        adjusted_question = self._adjust_question_to_style(base_question, style_features)
        
        return adjusted_question
    
    def _adjust_question_to_style(self, question: str, style_features: Dict) -> str:
        """ì§ˆë¬¸ì„ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ì¡°ì •"""
        adjusted = question
        
        # ì¡´ëŒ“ë§/ë°˜ë§ ì¡°ì •
        formality = style_features.get('formality', 'mixed')
        if formality == 'informal':
            # ì¡´ëŒ“ë§ì„ ë°˜ë§ë¡œ ë³€ê²½
            adjusted = adjusted.replace('ìš”', 'ì–´')
            adjusted = adjusted.replace('ë‹ˆë‹¤', 'ì–´')
            adjusted = adjusted.replace('ìŠµë‹ˆë‹¤', 'ì–´')
        elif formality == 'formal':
            # ë°˜ë§ì„ ì¡´ëŒ“ë§ë¡œ ë³€ê²½
            adjusted = adjusted.replace('ì–´', 'ìš”')
            adjusted = adjusted.replace('ì•„', 'ìš”')
        
        # ê°ì • í‘œí˜„ ì¶”ê°€
        emotion = style_features.get('emotion', 'neutral')
        if emotion == 'frustrated':
            adjusted = adjusted.replace('ìš”', 'ìš”...')
            adjusted = adjusted.replace('ì–´', 'ì–´...')
        elif emotion == 'anxious':
            adjusted = adjusted.replace('ìš”', 'ìš”?')
            adjusted = adjusted.replace('ì–´', 'ì–´?')
        elif emotion == 'desperate':
            if 'ì‹œí—˜' not in adjusted:
                adjusted = adjusted.replace('ìš”', 'ìš”! ì‹œí—˜ ë•Œë¬¸ì— ê¸‰í•´ìš”!')
                adjusted = adjusted.replace('ì–´', 'ì–´! ì‹œí—˜ ë•Œë¬¸ì— ê¸‰í•´!')
        
        # ë¶ˆí™•ì‹¤í•¨ í‘œí˜„ ì¶”ê°€
        uncertainty = style_features.get('uncertainty', 0)
        if uncertainty >= 2:
            if 'ê°™ì€ë°' not in adjusted:
                adjusted = adjusted.replace('ìš”', 'ìš”... ë§ëŠ” ê²ƒ ê°™ì€ë° í™•ì‹ ì´ ì•ˆ ì„œìš”')
                adjusted = adjusted.replace('ì–´', 'ì–´... ë§ëŠ” ê²ƒ ê°™ì€ë° í™•ì‹ ì´ ì•ˆ ì„œ')
        
        return adjusted
    
    async def generate_multiple_llm_questions(self, 
                                           target_topic: str,
                                           count: int = 5,
                                           style_variety: bool = True) -> List[str]:
        """LLMì„ í™œìš©í•´ ì—¬ëŸ¬ ê°œì˜ ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„±"""
        questions = []
        
        for i in range(count):
            if style_variety:
                # ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ë¡œ ìƒì„±
                available_styles = list(self.style_analysis.keys())
                style = random.choice(available_styles)
            else:
                style = None
            
            question = await self.generate_llm_deepfake_question(target_topic, style)
            questions.append(question)
        
        return questions
    
    def get_style_statistics(self) -> Dict[str, int]:
        """ìŠ¤íƒ€ì¼ë³„ í†µê³„"""
        stats = defaultdict(int)
        for features in self.style_analysis.values():
            stats[features.get('formality', 'unknown')] += 1
            stats[features.get('emotion', 'unknown')] += 1
        
        return dict(stats)
    
    def find_similar_style_questions(self, target_style: Dict, count: int = 5) -> List[str]:
        """íŠ¹ì • ìŠ¤íƒ€ì¼ê³¼ ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤ ì°¾ê¸°"""
        similarities = []
        
        for question, features in self.style_analysis.items():
            similarity_score = self._calculate_style_similarity(target_style, features)
            similarities.append((question, similarity_score))
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ countê°œ ë°˜í™˜
        return [q for q, s in similarities[:count]]
    
    def _calculate_style_similarity(self, style1: Dict, style2: Dict) -> float:
        """ë‘ ìŠ¤íƒ€ì¼ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not style1 or not style2:
            return 0.0
        
        score = 0.0
        total_features = 0
        
        # ì£¼ìš” íŠ¹ì§•ë“¤ ë¹„êµ
        key_features = ['formality', 'emotion', 'urgency', 'uncertainty']
        
        for feature in key_features:
            if feature in style1 and feature in style2:
                if style1[feature] == style2[feature]:
                    score += 1.0
                total_features += 1
        
        # ìˆ˜ì¹˜í˜• íŠ¹ì§•ë“¤ ë¹„êµ (ì •ê·œí™”ëœ ìœ ì‚¬ë„)
        numeric_features = ['length']
        for feature in numeric_features:
            if feature in style1 and feature in style2:
                val1, val2 = style1[feature], style2[feature]
                if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
                    # ê¸¸ì´ì˜ ê²½ìš° ì •ê·œí™”ëœ ìœ ì‚¬ë„ ê³„ì‚°
                    max_len = max(val1, val2)
                    if max_len > 0:
                        similarity = 1.0 - abs(val1 - val2) / max_len
                        score += similarity
                    total_features += 1
        
        return score / total_features if total_features > 0 else 0.0

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¤– LLM ê¸°ë°˜ ë”¥í˜ì´í¬ì‹ í•™ìƒ ì§ˆë¬¸ ìƒì„±ê¸° ì‹œì‘\n")
    
    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    print("ğŸ”§ í™˜ê²½ë³€ìˆ˜ í™•ì¸:")
    api_key = os.getenv('OPENAI_API_KEY')
    model = os.getenv('OPENAI_MODEL', 'gpt-4')
    data_path = os.getenv('DATA_PATH', 'data/evaluation_statistics.json')
    log_level = os.getenv('LOG_LEVEL', 'INFO')
    
    print(f"  OpenAI API í‚¤: {'âœ… ì„¤ì •ë¨' if api_key else 'âŒ ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
    print(f"  LLM ëª¨ë¸: {model}")
    print(f"  ë°ì´í„° ê²½ë¡œ: {data_path}")
    print(f"  ë¡œê·¸ ë ˆë²¨: {log_level}")
    print()
    
    if not api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì„¤ì •í•˜ì„¸ìš”:")
        print("1. .env íŒŒì¼ì— OPENAI_API_KEY=your-api-key ì¶”ê°€")
        print("2. export OPENAI_API_KEY='your-api-key' ì‹¤í–‰")
        print("3. ì½”ë“œì—ì„œ ì§ì ‘ ì „ë‹¬")
        return
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = LLMDeepfakeGenerator(api_key, model, data_path)
    
    if not generator.real_questions:
        print("âŒ ì‹¤ì œ ì§ˆë¬¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… {len(generator.real_questions)}ê°œì˜ ì‹¤ì œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ\n")
    
    # ìŠ¤íƒ€ì¼ í†µê³„ ì¶œë ¥
    style_stats = generator.get_style_statistics()
    print("ğŸ“Š ìŠ¤íƒ€ì¼ë³„ í†µê³„:")
    for style, count in style_stats.items():
        print(f"  {style}: {count}ê°œ")
    
    print("\n" + "="*60 + "\n")
    
    # LLMì„ í™œìš©í•œ ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„±
    topics = ['ìˆ˜ì—´', 'ì í™”ì‹', 'ê·€ë‚©ë²•', 'ìˆ˜ì—´ì˜í•©']
    
    for topic in topics:
        print(f"ğŸ¯ ì£¼ì œ: {topic}")
        print("-" * 40)
        
        # ë‹¤ì–‘í•œ ë‚œì´ë„ë¡œ ì§ˆë¬¸ ìƒì„±
        difficulties = ['naive', 'basic', 'intermediate', 'advanced']
        
        for difficulty in difficulties:
            question = await generator.generate_llm_deepfake_question(topic, difficulty=difficulty)
            print(f"{difficulty:12}: {question}")
        
        print()
    
    print("="*60 + "\n")
    
    # ìŠ¤íƒ€ì¼ë³„ ì§ˆë¬¸ ìƒì„± ì˜ˆì‹œ
    print("ğŸ­ ìŠ¤íƒ€ì¼ë³„ ì§ˆë¬¸ ìƒì„± ì˜ˆì‹œ:")
    print("-" * 40)
    
    # ì‹¤ì œ ìŠ¤íƒ€ì¼ ì¤‘ì—ì„œ ì„ íƒ
    available_styles = list(generator.style_analysis.keys())[:5]  # ì²˜ìŒ 5ê°œë§Œ
    
    for style in available_styles:
        question = await generator.generate_llm_deepfake_question('ìˆ˜ì—´', style)
        print(f"ìŠ¤íƒ€ì¼: {question[:50]}...")
    
    print("\n" + "="*60 + "\n")
    
    # ì—¬ëŸ¬ ì§ˆë¬¸ í•œë²ˆì— ìƒì„±
    print("ğŸ”„ ì—¬ëŸ¬ ì§ˆë¬¸ í•œë²ˆì— ìƒì„±:")
    print("-" * 40)
    
    multiple_questions = await generator.generate_multiple_llm_questions('ì í™”ì‹', count=3, style_variety=True)
    
    for i, question in enumerate(multiple_questions, 1):
        print(f"{i}. {question}")
    
    print("\nâœ… LLM ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main())

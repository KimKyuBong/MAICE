#!/usr/bin/env python3
"""
ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„±ê¸° ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import asyncio
from pathlib import Path

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install python-dotenvë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    print("í™˜ê²½ë³€ìˆ˜ë¥¼ ì§ì ‘ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

def check_environment():
    """í™˜ê²½ ì„¤ì • í™•ì¸"""
    print("ğŸ”§ í™˜ê²½ ì„¤ì • í™•ì¸:")
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv('OPENAI_API_KEY')
    print(f"  OpenAI API í‚¤: {'âœ… ì„¤ì •ë¨' if api_key else 'âŒ ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
    
    # LLM ëª¨ë¸ í™•ì¸
    model = os.getenv('OPENAI_MODEL', 'gpt-5-mini')
    print(f"  LLM ëª¨ë¸: {model}")
    
    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    data_path = os.getenv('DATA_PATH', 'data/evaluation_statistics.json')
    print(f"  ë°ì´í„° ê²½ë¡œ: {data_path}")
    
    # ë¡œê·¸ ë ˆë²¨ í™•ì¸
    log_level = os.getenv('LOG_LEVEL', 'INFO')
    print(f"  ë¡œê·¸ ë ˆë²¨: {log_level}")
    
    # ìµœëŒ€ ì§ˆë¬¸ ìˆ˜ í™•ì¸
    max_questions = os.getenv('MAX_QUESTIONS', '1000')
    print(f"  ìµœëŒ€ ì§ˆë¬¸ ìˆ˜: {max_questions}")
    
    print()
    return api_key, model, data_path

async def run_llm_generator():
    """LLM ê¸°ë°˜ ë”¥í˜ì´í¬ ìƒì„±ê¸° ì‹¤í–‰"""
    try:
        from llm_deepfake_generator import LLMDeepfakeGenerator
        
        print("ğŸš€ LLM ê¸°ë°˜ ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„±ê¸° ì‹¤í–‰ ì¤‘...\n")
        
        # í™˜ê²½ ì„¤ì • í™•ì¸
        api_key, model, data_path = check_environment()
        
        if not api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ë‹¤ìŒ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì„¤ì •í•˜ì„¸ìš”:")
            print("1. .env íŒŒì¼ì— OPENAI_API_KEY=your-api-key ì¶”ê°€")
            print("2. export OPENAI_API_KEY='your-api-key' ì‹¤í–‰")
            print("3. ì½”ë“œì—ì„œ ì§ì ‘ ì „ë‹¬")
            return
        
        # ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = LLMDeepfakeGenerator(api_key, model, data_path)
        
        if not generator.real_questions:
            print("âŒ ì‹¤ì œ ì§ˆë¬¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… {len(generator.real_questions)}ê°œì˜ ì‹¤ì œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ\n")
        
        # ìŠ¤íƒ€ì¼ í†µê³„ ì¶œë ¥
        style_stats = generator.get_style_statistics()
        print("ğŸ“Š ìŠ¤íƒ€ì¼ë³„ í†µê³„:")
        for style, count in style_stats.items():
            print(f"  {style}: {count}ê°œ")
        
        print("\n" + "="*60 + "\n")
        
        # ë‹¤ì–‘í•œ ì£¼ì œë¡œ ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„±
        topics = ['ìˆ˜ì—´', 'ì í™”ì‹', 'ê·€ë‚©ë²•', 'ìˆ˜ì—´ì˜í•©']
        
        for topic in topics:
            print(f"ğŸ¯ ì£¼ì œ: {topic}")
            print("-" * 40)
            
            # ë‹¤ì–‘í•œ ë‚œì´ë„ë¡œ ì§ˆë¬¸ ìƒì„±
            difficulties = ['naive', 'basic', 'intermediate', 'advanced']
            
            for difficulty in difficulties:
                question = await generator.generate_llm_deepfake_question(topic, difficulty=difficulty)
                print(f"{difficulty:12}: {question}")
            
            print()
        
        print("="*60 + "\n")
        
        # ìŠ¤íƒ€ì¼ë³„ ì§ˆë¬¸ ìƒì„± ì˜ˆì‹œ
        print("ğŸ­ ìŠ¤íƒ€ì¼ë³„ ì§ˆë¬¸ ìƒì„± ì˜ˆì‹œ:")
        print("-" * 40)
        
        # ì‹¤ì œ ìŠ¤íƒ€ì¼ ì¤‘ì—ì„œ ì„ íƒ
        available_styles = list(generator.style_analysis.keys())[:5]  # ì²˜ìŒ 5ê°œë§Œ
        
        for style in available_styles:
            question = await generator.generate_llm_deepfake_question('ìˆ˜ì—´', style)
            print(f"ìŠ¤íƒ€ì¼: {question[:50]}...")
        
        print("\n" + "="*60 + "\n")
        
        # ì—¬ëŸ¬ ì§ˆë¬¸ í•œë²ˆì— ìƒì„±
        print("ğŸ”„ ì—¬ëŸ¬ ì§ˆë¬¸ í•œë²ˆì— ìƒì„±:")
        print("-" * 40)
        
        multiple_questions = await generator.generate_multiple_llm_questions('ì í™”ì‹', count=3, style_variety=True)
        
        for i, question in enumerate(multiple_questions, 1):
            print(f"{i}. {question}")
        
        print("\nâœ… LLM ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ!")
        
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
        print("í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install openai")
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def run_basic_generator():
    """ê¸°ë³¸ ë”¥í˜ì´í¬ ìƒì„±ê¸° ì‹¤í–‰"""
    try:
        from deepfake_question_generator import DeepfakeQuestionGenerator
        
        print("ğŸš€ ê¸°ë³¸ ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„±ê¸° ì‹¤í–‰ ì¤‘...\n")
        
        # í™˜ê²½ ì„¤ì • í™•ì¸
        _, _, data_path = check_environment()
        
        # ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = DeepfakeQuestionGenerator(data_path)
        
        if not generator.real_questions:
            print("âŒ ì‹¤ì œ ì§ˆë¬¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… {len(generator.real_questions)}ê°œì˜ ì‹¤ì œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ\n")
        
        # ìŠ¤íƒ€ì¼ í†µê³„ ì¶œë ¥
        style_stats = generator.get_style_statistics()
        print("ğŸ“Š ìŠ¤íƒ€ì¼ë³„ í†µê³„:")
        for style, count in style_stats.items():
            print(f"  {style}: {count}ê°œ")
        
        print()
        
        # ì£¼ì œë³„ í†µê³„ ì¶œë ¥
        topic_stats = generator.get_topic_statistics()
        print("ğŸ“š ì£¼ì œë³„ í†µê³„:")
        for topic, count in topic_stats.items():
            print(f"  {topic}: {count}ê°œ")
        
        print("\n" + "="*60 + "\n")
        
        # ë‹¤ì–‘í•œ ì£¼ì œë¡œ ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„±
        topics = ['ìˆ˜ì—´', 'ì í™”ì‹', 'ê·€ë‚©ë²•', 'ìˆ˜ì—´ì˜í•©']
        
        for topic in topics:
            print(f"ğŸ¯ ì£¼ì œ: {topic}")
            print("-" * 40)
            
            # ë‹¤ì–‘í•œ ë‚œì´ë„ë¡œ ì§ˆë¬¸ ìƒì„±
            difficulties = ['naive', 'basic', 'intermediate', 'advanced', 'olympiad']
            
            for difficulty in difficulties:
                question = generator.generate_deepfake_question(topic, difficulty=difficulty)
                print(f"{difficulty:12}: {question}")
            
            print()
        
        print("="*60 + "\n")
        
        # ìŠ¤íƒ€ì¼ë³„ ì§ˆë¬¸ ìƒì„± ì˜ˆì‹œ
        print("ğŸ­ ìŠ¤íƒ€ì¼ë³„ ì§ˆë¬¸ ìƒì„± ì˜ˆì‹œ:")
        print("-" * 40)
        
        styles = ['formal', 'informal', 'emoji', 'ellipsis', 'uncertain', 'urgent']
        
        for style in styles:
            if style in generator.style_patterns and generator.style_patterns[style]:
                question = generator.generate_deepfake_question('ìˆ˜ì—´', style)
                print(f"{style:10}: {question}")
        
        print("\n" + "="*60 + "\n")
        
        # ì—¬ëŸ¬ ì§ˆë¬¸ í•œë²ˆì— ìƒì„±
        print("ğŸ”„ ì—¬ëŸ¬ ì§ˆë¬¸ í•œë²ˆì— ìƒì„±:")
        print("-" * 40)
        
        multiple_questions = generator.generate_multiple_questions('ì í™”ì‹', count=3, style_variety=True)
        
        for i, question in enumerate(multiple_questions, 1):
            print(f"{i}. {question}")
        
        print("\nâœ… ê¸°ë³¸ ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ!")
        
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

async def run_auto_test():
    """ìë™ìœ¼ë¡œ 300ê°œ ì§ˆë¬¸(30ê°œ x 10íšŒ)ì„ ìƒì„±í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    print("ğŸš€ ìë™ ë”¥í˜ì´í¬ ì§ˆë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        # í•„ìš”í•œ ëª¨ë“ˆë“¤ import
        from llm_deepfake_generator import LLMDeepfakeGenerator
        import json
        import random
        import os
        import re
        from datetime import datetime
        import asyncio
        
        # LLMDeepfakeGenerator ë¡œë“œ (ëª¨ë¸ ëª…ì‹œ: gpt-5-mini)
        model_env = os.getenv('OPENAI_MODEL', 'gpt-5-mini')
        generator = LLMDeepfakeGenerator(model=model_env)
        print("âœ… LLMDeepfakeGenerator ë¡œë“œ ì™„ë£Œ")
        
        # 1ë‹¨ê³„: ì‹¤ì œ ì§ˆë¬¸ì—ì„œ 20ê°œë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ
        print("ğŸ¯ 1ë‹¨ê³„: ì‹¤ì œ ì§ˆë¬¸ì—ì„œ 20ê°œë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ ì¤‘...")
        print("-" * 50)
        
        real_questions = generator.real_questions
        if len(real_questions) < 20:
            print(f"âŒ ì‹¤ì œ ì§ˆë¬¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. (í˜„ì¬: {len(real_questions)}ê°œ, í•„ìš”: 20ê°œ)")
            return
        
        # 20ê°œ ëœë¤ ì„ íƒ (ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ì§ì ‘ ì‚¬ìš©)
        input_questions = random.sample(real_questions, 20)
        for i, q in enumerate(input_questions, 1):
            print(f"  {i:2d}. {q[:60]}...")
        print(f"âœ… 20ê°œ ì‹¤ì œ ì§ˆë¬¸ ì„ íƒ ì™„ë£Œ\n")
        
        # 2ë‹¨ê³„: ì„ íƒëœ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ 300ê°œ ë³€í˜• ì§ˆë¬¸ì„ ë°°ì¹˜(30ê°œ x 10íšŒ)ë¡œ ìƒì„±
        print("ğŸ¯ 2ë‹¨ê³„: ì„ íƒëœ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ 300ê°œ ë³€í˜• ì§ˆë¬¸ì„ ë°°ì¹˜(30ê°œ x 10íšŒ)ë¡œ ìƒì„± ì¤‘...")
        print("-" * 50)
        
        # ìˆ˜í•™ ì£¼ì œë“¤
        math_topics = ['ìˆ˜ì—´', 'ì í™”ì‹', 'ê·€ë‚©ë²•', 'ìˆ˜ì—´ì˜í•©', 'ë“±ì°¨ìˆ˜ì—´', 'ë“±ë¹„ìˆ˜ì—´', 'ìˆ˜í•™ì ê·€ë‚©ë²•', 'ì¡°í•©ë¡ ']
        
        # 300ê°œ ì§ˆë¬¸ì„ 30ê°œì”© 10ë²ˆì— ë‚˜ëˆ„ì–´ ìƒì„±
        batch_size = 30
        total_batches = 10
        all_output_questions = []
        next_global_id = 1
        
        for batch_num in range(total_batches):
            print(f"ğŸ“¦ ë°°ì¹˜ {batch_num + 1}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            
            # í˜„ì¬ ë°°ì¹˜ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
            batch_prompts = []
            for i in range(batch_size):
                batch_idx = batch_num * batch_size + i
                input_idx = batch_idx % len(input_questions)
                input_q = input_questions[input_idx]
                target_topic = random.choice(math_topics)
                
                batch_prompts.append({
                    'id': next_global_id + i,
                    'input_id': input_idx + 1,
                    'original_question': input_q,
                    'target_topic': target_topic
                })
            
            # ë°°ì¹˜ìš© JSON í”„ë¡¬í”„íŠ¸ ìƒì„± (ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜ ì§€ì‹œ)
            combined_prompt = json.dumps({
                "request_type": "batch_question_generation",
                "num_outputs": batch_size,
                "topics": math_topics,
                "input_questions_context": input_questions,
                "output_contract": {
                    "type": "object",
                    "properties": {
                        "generated_questions": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "id": {"type": "integer"},
                                    "target_topic": {"type": "string"},
                                    "new_question": {"type": "string"}
                                },
                                "required": ["id", "target_topic", "new_question"]
                            }
                        }
                    },
                    "required": ["generated_questions"]
                },
                "instructions": "ì…ë ¥ëœ 20ê°œ ì§ˆë¬¸ì€ í•œ ë°˜ í•™ìƒë“¤ì´ ì‹¤ì œë¡œ í•œ ì§ˆë¬¸ë“¤ì…ë‹ˆë‹¤. ì´ í•™ìƒë“¤ì˜ ì§ˆë¬¸ ìŠ¤íƒ€ì¼, ì–´ì¡°, íŒ¨í„´ì„ í•™ìŠµí•´ì„œ ì™„ì „íˆ ìƒˆë¡œìš´ 30ê°œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”. ê¸°ì¡´ ì§ˆë¬¸ì„ ë³€í˜•í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ê°™ì€ ë°˜ í•™ìƒì´ ë¬¼ì–´ë³¼ ë²•í•œ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”. ìˆ˜í•™ ì£¼ì œëŠ” topics ë¦¬ìŠ¤íŠ¸ì—ì„œ ì„ íƒí•˜ê³ , ê³ ë“±í•™êµ ìˆ˜ì¤€ì— ë§ê²Œ í•˜ì„¸ìš”. ì˜¤ì§ í•˜ë‚˜ì˜ JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ì„¸ìš”."
            }, ensure_ascii=False, indent=2)
            
            try:
                print(f"ğŸš€ GPT-5-miniì— {batch_size}ê°œ ì§ˆë¬¸(JSON)ì„ ì „ì†¡ ì¤‘...")
                
                response = await generator.openai_client.chat.completions.create(
                    model=generator.model,
                    messages=[
                        {"role": "system", "content": "ë„ˆëŠ” JSONë§Œ ë°˜í™˜í•˜ëŠ” ìƒì„± ì—ì´ì „íŠ¸ë‹¤. ì„¤ëª…ì´ë‚˜ ìì—°ì–´ ê¸ˆì§€. ì˜¤ì§ í•˜ë‚˜ì˜ JSON ê°ì²´ë§Œ ì¶œë ¥."},
                        {"role": "user", "content": combined_prompt}
                    ],
                    max_completion_tokens=10000
                )
                
                # ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ì§ˆë¬¸ ì¶”ì¶œ
                response_text = response.choices[0].message.content.strip()
                print(f"ğŸ“ GPT ì‘ë‹µ ë°›ìŒ (ê¸¸ì´: {len(response_text)}ì)")
                
                # JSON ê°•ê±´ íŒŒì‹±: ë°”ë¡œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì‘ë‹µì—ì„œ JSON ê°ì²´ë§Œ ì¶”ì¶œ
                def robust_parse_json(text: str):
                    try:
                        return json.loads(text)
                    except Exception:
                        pass
                    m = re.search(r"\{[\s\S]*\}", text)
                    if m:
                        try:
                            return json.loads(m.group(0))
                        except Exception:
                            return {}
                    return {}
                
                response_data = robust_parse_json(response_text)
                batch_output_questions = []
                
                # 'generated_questions' ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ 'outputs' ë“± ëŒ€ì²´ í‚¤ ì‹œë„
                generated_list = response_data.get('generated_questions')
                if not isinstance(generated_list, list):
                    generated_list = response_data.get('outputs')
                if not isinstance(generated_list, list):
                    generated_list = []
                
                # id ë§¤í•‘ì´ ìˆìœ¼ë©´ í™œìš©, ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ ë§¤í•‘
                generated_map = {}
                for item in generated_list:
                    if isinstance(item, dict) and 'id' in item:
                        generated_map[item['id']] = item
                
                for i, spec in enumerate(batch_prompts):
                    # id ê¸°ë°˜ ìš°ì„ , ì—†ìœ¼ë©´ ìˆœì„œ ê¸°ë°˜
                    generated_q = generated_map.get(spec['id'])
                    if not generated_q and i < len(generated_list) and isinstance(generated_list[i], dict):
                        generated_q = generated_list[i]
                    new_question_text = generated_q.get('new_question') if isinstance(generated_q, dict) else ""
                    target_topic = generated_q.get('target_topic') if isinstance(generated_q, dict) and generated_q.get('target_topic') else spec['target_topic']
                    
                    batch_output_questions.append({
                        'id': spec['id'],
                        'input_id': spec['input_id'],
                        'original_question': spec['original_question'],
                        'target_topic': target_topic,
                        'new_question': new_question_text if new_question_text else f"[ìƒì„± ì‹¤íŒ¨] {spec['original_question']}",
                        'timestamp': datetime.now().isoformat()
                    })
                
                # í˜„ì¬ ë°°ì¹˜ ê²°ê³¼ ì¶œë ¥ (ì²˜ìŒ 5ê°œ ë¯¸ë¦¬ë³´ê¸°)
                for preview in batch_output_questions[:5]:
                    print(f"  {preview['id']:3d}. [{preview['target_topic']}] {preview['new_question'][:50]}...")
                if len(batch_output_questions) > 5:
                    print(f"  ... ì™¸ {len(batch_output_questions) - 5}ê°œ")
                
                all_output_questions.extend(batch_output_questions)
                next_global_id += batch_size
                print(f"âœ… ë°°ì¹˜ {batch_num + 1} ì™„ë£Œ ({len(batch_output_questions)}ê°œ)")
                
                # API ì œí•œ ê³ ë ¤í•˜ì—¬ ì ì‹œ ëŒ€ê¸°
                if batch_num < total_batches - 1:
                    print("â³ ë‹¤ìŒ ë°°ì¹˜ ì¤€ë¹„ ì¤‘... (2ì´ˆ ëŒ€ê¸°)")
                    await asyncio.sleep(2)
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_num + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ì— ëŒ€í•´ ê¸°ë³¸ ì§ˆë¬¸ ìƒì„±
                for spec in batch_prompts:
                    all_output_questions.append({
                        'id': spec['id'],
                        'input_id': spec['input_id'],
                        'original_question': spec['original_question'],
                        'target_topic': spec['target_topic'],
                        'new_question': f"[ë°°ì¹˜ {batch_num + 1} ìƒì„± ì‹¤íŒ¨] {spec['original_question']}",
                        'timestamp': datetime.now().isoformat()
                    })
                next_global_id += batch_size
        
        print(f"âœ… 300ê°œ ë³€í˜• ì§ˆë¬¸ ìƒì„± ì™„ë£Œ (ì‹¤ì œ ìƒì„±: {len(all_output_questions)}ê°œ)\n")
        
        # 3ë‹¨ê³„: ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        print("ğŸ¯ 3ë‹¨ê³„: ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
        print("-" * 50)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (tester ë””ë ‰í† ë¦¬ ì•ˆì—)
        output_dir = 'output'
        os.makedirs(output_dir, exist_ok=True)
        
        # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        result_data = {
            'test_info': {
                'timestamp': datetime.now().isoformat(),
                'model': generator.model,
                'input_count': len(input_questions),
                'output_count': len(all_output_questions),
                'batch_size': batch_size,
                'total_batches': total_batches
            },
            'input_questions': input_questions,
            'output_questions': all_output_questions
        }
        
        # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'{output_dir}/deepfake_test_results_{timestamp}.json'
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“Š ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(all_output_questions)}ê°œ")
        print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {os.path.getsize(filename) / 1024:.1f} KB")
        
        # í’ˆì§ˆ í†µê³„ ì¶œë ¥
        print("\nğŸ“ˆ í’ˆì§ˆ í†µê³„:")
        topic_counts = {}
        success_count = 0
        for q in all_output_questions:
            topic = q['target_topic']
            topic_counts[topic] = topic_counts.get(topic, 0) + 1
            if not q['new_question'].startswith('[ìƒì„± ì‹¤íŒ¨]') and not q['new_question'].startswith('[ë°°ì¹˜'):
                success_count += 1
        
        print(f"  ì„±ê³µë¥ : {success_count}/{len(all_output_questions)} ({success_count/len(all_output_questions)*100:.1f}%)")
        print("  ì£¼ì œë³„ ë¶„í¬:")
        for topic, count in sorted(topic_counts.items()):
            print(f"    {topic}: {count}ê°œ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def show_environment_help():
    """í™˜ê²½ ì„¤ì • ë„ì›€ë§ í‘œì‹œ"""
    print("\nğŸ“– í™˜ê²½ ì„¤ì • ë„ì›€ë§")
    print("="*40)
    print("1. .env íŒŒì¼ ìƒì„±:")
    print("   tester í´ë”ì— .env íŒŒì¼ì„ ë§Œë“¤ê³  ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”:")
    print()
    print("   OPENAI_API_KEY=your-openai-api-key-here")
    print("   OPENAI_MODEL=gpt-5-mini")
    print("   DATA_PATH=data/evaluation_statistics.json")
    print("   LOG_LEVEL=INFO")
    print("   MAX_QUESTIONS=1000")
    print()
    print("2. í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì„¤ì •:")
    print("   export OPENAI_API_KEY='your-api-key'")
    print("   export OPENAI_MODEL='gpt-5-mini'")
    print()
    print("3. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:")
    print("   pip install -r requirements.txt")
    print()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ­ ë”¥í˜ì´í¬ì‹ í•™ìƒ ì§ˆë¬¸ ìƒì„±ê¸° (ìë™í™” ëª¨ë“œ)\n")
    
    # í™˜ê²½ ì„¤ì • í™•ì¸
    api_key, _, _ = check_environment()
    
    if not api_key:
        print("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ìë™í™” í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ API í‚¤ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        print("ë‹¤ìŒ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì„¤ì •í•˜ì„¸ìš”:")
        print("1. .env íŒŒì¼ì— OPENAI_API_KEY=your-api-key ì¶”ê°€")
        print("2. export OPENAI_API_KEY='your-api-key' ì‹¤í–‰")
        return
    
    print("ğŸš€ ìë™í™”ëœ ë”¥í˜ì´í¬ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“‹ ì‹¤ì œ í•™ìƒ ì§ˆë¬¸ 20ê°œ â†’ ë³€í˜• ì§ˆë¬¸ 30ê°œ â†’ JSON íŒŒì¼ ì €ì¥")
    print("="*60)
    
    try:
        # ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        asyncio.run(run_auto_test())
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
ì§ˆë¬¸ë¶„ë¥˜ ë° ëª…ë£Œí™” ë‹¨ê³„ í…ŒìŠ¤í„°
ìƒì„±ëœ 300ê°œ ë”¥í˜ì´í¬ ì§ˆë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ì§ˆë¬¸ë¶„ë¥˜ ë° ëª…ë£Œí™” ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import logging
import os
import random
import time
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸
try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logging.warning("openaië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install openai'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('question_classification_tester.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class QuestionClassificationTester:
    """ì§ˆë¬¸ë¶„ë¥˜ ë° ëª…ë£Œí™” ë‹¨ê³„ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.openai_client = None
        self.model = os.getenv('OPENAI_MODEL', 'gpt-5-mini')
        self.test_questions = []
        self.results = []
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if OPENAI_AVAILABLE:
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                self.openai_client = AsyncOpenAI(api_key=api_key)
                logging.info(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")
            else:
                logging.warning("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        else:
            logging.warning("âŒ OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    def load_test_questions(self, file_path: str) -> bool:
        """í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ ë°ì´í„° ë¡œë“œ"""
        try:
            if not os.path.exists(file_path):
                logging.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return False
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if 'output_questions' not in data:
                logging.error("output_questions í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ì„±ê³µì ìœ¼ë¡œ ìƒì„±ëœ ì§ˆë¬¸ë§Œ í•„í„°ë§
            valid_questions = []
            for q in data['output_questions']:
                if (isinstance(q, dict) and 
                    'new_question' in q and 
                    q['new_question'] and 
                    not q['new_question'].startswith('[ìƒì„± ì‹¤íŒ¨]') and
                    not q['new_question'].startswith('[ë°°ì¹˜')):
                    valid_questions.append(q)
            
            self.test_questions = valid_questions
            logging.info(f"âœ… {len(self.test_questions)}ê°œì˜ ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logging.error(f"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def test_question_classification(self, question: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œë³„ ì§ˆë¬¸ì˜ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"""
        try:
            # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ë¥˜
            classification_prompt = f"""
ë‹¤ìŒ í•™ìƒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í•­ëª©ë“¤ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {question['new_question']}

ë¶„ë¥˜ í•­ëª©:
1. ìˆ˜í•™ ì£¼ì œ (ìˆ˜ì—´, ì í™”ì‹, ê·€ë‚©ë²•, ìˆ˜ì—´ì˜í•©, ë“±ì°¨ìˆ˜ì—´, ë“±ë¹„ìˆ˜ì—´, ìˆ˜í•™ì ê·€ë‚©ë²•, ì¡°í•©ë¡  ì¤‘ ì„ íƒ)
2. ì§ˆë¬¸ ìœ í˜• (ê°œë… ì´í•´, ê³„ì‚° ë°©ë²•, ì¦ëª…, ë¬¸ì œ í’€ì´, ì˜¤ê°œë… í™•ì¸ ë“±)
3. ë‚œì´ë„ (ì´ˆê¸‰, ì¤‘ê¸‰, ê³ ê¸‰)
4. í•™ìƒ ìˆ˜ì¤€ (ê¸°ì´ˆ, ë³´í†µ, ì‹¬í™”)
5. ëª…ë£Œí™” í•„ìš” ì—¬ë¶€ (ì˜ˆ/ì•„ë‹ˆì˜¤)
6. ëª…ë£Œí™”ê°€ í•„ìš”í•œ ì´ìœ  (ëª…ë£Œí™”ê°€ í•„ìš”í•˜ë‹¤ë©´)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "classification": {{
        "topic": "ì£¼ì œëª…",
        "question_type": "ì§ˆë¬¸ìœ í˜•",
        "difficulty": "ë‚œì´ë„",
        "student_level": "í•™ìƒìˆ˜ì¤€",
        "needs_clarification": true/false,
        "clarification_reason": "ì´ìœ "
    }}
}}
"""
            
            response = await self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ìƒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì¼ê´€ëœ ë¶„ë¥˜ë¥¼ ì œê³µí•˜ì„¸ìš”."},
                    {"role": "user", "content": classification_prompt}
                ],
                max_completion_tokens=1000
            )
            
            classification_result = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹±
            try:
                classification_data = json.loads(classification_result)
                classification = classification_data.get('classification', {})
            except:
                classification = {"error": "JSON íŒŒì‹± ì‹¤íŒ¨"}
            
            # 2ë‹¨ê³„: ëª…ë£Œí™” ì§ˆë¬¸ ìƒì„± (í•„ìš”í•œ ê²½ìš°)
            clarification_questions = []
            if classification.get('needs_clarification', False):
                clarification_prompt = f"""
ë‹¤ìŒ í•™ìƒ ì§ˆë¬¸ì— ëŒ€í•´ ëª…ë£Œí™”ê°€ í•„ìš”í•©ë‹ˆë‹¤:

ì›ë³¸ ì§ˆë¬¸: {question['new_question']}
ë¶„ë¥˜: {classification.get('topic', '')} - {classification.get('question_type', '')}
ëª…ë£Œí™” ì´ìœ : {classification.get('clarification_reason', '')}

ì´ í•™ìƒì—ê²Œ ëª…ë£Œí™”ë¥¼ ìœ„í•´ ë¬¼ì–´ë³¼ 2-3ê°œì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ê° ì§ˆë¬¸ì€ í•™ìƒì´ ìì‹ ì˜ ì˜ë„ë¥¼ ë” ëª…í™•í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì•¼ í•©ë‹ˆë‹¤.

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "clarification_questions": [
        "ëª…ë£Œí™” ì§ˆë¬¸ 1",
        "ëª…ë£Œí™” ì§ˆë¬¸ 2",
        "ëª…ë£Œí™” ì§ˆë¬¸ 3"
    ]
}}
"""
                
                clarification_response = await self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ìƒ ì§ˆë¬¸ì„ ëª…ë£Œí™”í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. êµ¬ì²´ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” ëª…ë£Œí™” ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."},
                        {"role": "user", "content": clarification_prompt}
                    ],
                    max_completion_tokens=1000
                )
                
                clarification_text = clarification_response.choices[0].message.content.strip()
                try:
                    clarification_data = json.loads(clarification_text)
                    clarification_questions = clarification_data.get('clarification_questions', [])
                except:
                    clarification_questions = ["ëª…ë£Œí™” ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨"]
            
            return {
                'question_id': question['id'],
                'original_question': question['new_question'],
                'target_topic': question['target_topic'],
                'classification': classification,
                'clarification_questions': clarification_questions,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logging.error(f"ì§ˆë¬¸ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ID: {question.get('id', 'unknown')}): {e}")
            return {
                'question_id': question.get('id', 'unknown'),
                'original_question': question.get('new_question', ''),
                'target_topic': question.get('target_topic', ''),
                'classification': {"error": str(e)},
                'clarification_questions': [],
                'timestamp': datetime.now().isoformat()
            }
    
    async def run_batch_test(self, batch_size: int = 10, delay: float = 1.0) -> None:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if not self.test_questions:
            logging.error("í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return
        
        total_questions = len(self.test_questions)
        total_batches = (total_questions + batch_size - 1) // batch_size
        
        logging.info(f"ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘: ì´ {total_questions}ê°œ ì§ˆë¬¸, {total_batches}ê°œ ë°°ì¹˜")
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, total_questions)
            batch_questions = self.test_questions[start_idx:end_idx]
            
            logging.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({start_idx+1}-{end_idx})")
            
            batch_results = []
            for i, question in enumerate(batch_questions):
                logging.info(f"  ì§ˆë¬¸ {start_idx + i + 1}/{total_questions} ì²˜ë¦¬ ì¤‘...")
                result = await self.test_question_classification(question)
                batch_results.append(result)
                
                # API ì œí•œ ê³ ë ¤í•˜ì—¬ ì ì‹œ ëŒ€ê¸°
                if i < len(batch_questions) - 1:
                    await asyncio.sleep(0.5)
            
            self.results.extend(batch_results)
            
            # ë°°ì¹˜ ì™„ë£Œ í›„ ì ì‹œ ëŒ€ê¸°
            if batch_num < total_batches - 1:
                logging.info(f"â³ ë‹¤ìŒ ë°°ì¹˜ ì¤€ë¹„ ì¤‘... ({delay}ì´ˆ ëŒ€ê¸°)")
                await asyncio.sleep(delay)
        
        logging.info(f"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(self.results)}ê°œ ê²°ê³¼")
    
    def analyze_results(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        if not self.results:
            return {"error": "ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ê¸°ë³¸ í†µê³„
        total_questions = len(self.results)
        successful_classifications = sum(1 for r in self.results if 'error' not in r.get('classification', {}))
        needs_clarification = sum(1 for r in self.results if r.get('classification', {}).get('needs_clarification', False))
        
        # ì£¼ì œë³„ ë¶„í¬
        topic_distribution = {}
        for result in self.results:
            topic = result.get('classification', {}).get('topic', 'unknown')
            topic_distribution[topic] = topic_distribution.get(topic, 0) + 1
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬
        question_type_distribution = {}
        for result in self.results:
            q_type = result.get('classification', {}).get('question_type', 'unknown')
            question_type_distribution[q_type] = question_type_distribution.get(q_type, 0) + 1
        
        # ë‚œì´ë„ë³„ ë¶„í¬
        difficulty_distribution = {}
        for result in self.results:
            difficulty = result.get('classification', {}).get('difficulty', 'unknown')
            difficulty_distribution[difficulty] = difficulty_distribution.get(difficulty, 0) + 1
        
        # í•™ìƒ ìˆ˜ì¤€ë³„ ë¶„í¬
        student_level_distribution = {}
        for result in self.results:
            level = result.get('classification', {}).get('student_level', 'unknown')
            student_level_distribution[level] = student_level_distribution.get(level, 0) + 1
        
        return {
            'summary': {
                'total_questions': total_questions,
                'successful_classifications': successful_classifications,
                'classification_success_rate': successful_classifications / total_questions * 100,
                'needs_clarification': needs_clarification,
                'clarification_rate': needs_clarification / total_questions * 100
            },
            'distributions': {
                'topics': topic_distribution,
                'question_types': question_type_distribution,
                'difficulties': difficulty_distribution,
                'student_levels': student_level_distribution
            }
        }
    
    def save_results(self, filename: str = None) -> str:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'output/question_classification_test_results_{timestamp}.json'
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('output', exist_ok=True)
        
        # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        result_data = {
            'test_info': {
                'timestamp': datetime.now().isoformat(),
                'model': self.model,
                'total_questions': len(self.test_questions),
                'tested_questions': len(self.results)
            },
            'analysis': self.analyze_results(),
            'detailed_results': self.results
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        logging.info(f"âœ… ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        return filename
    
    def print_summary(self) -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        analysis = self.analyze_results()
        
        if 'error' in analysis:
            logging.error(f"ë¶„ì„ ì˜¤ë¥˜: {analysis['error']}")
            return
        
        summary = analysis['summary']
        distributions = analysis['distributions']
        
        print("\n" + "="*60)
        print("ğŸ“Š ì§ˆë¬¸ë¶„ë¥˜ ë° ëª…ë£Œí™” í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        print(f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„:")
        print(f"  ì´ ì§ˆë¬¸ ìˆ˜: {summary['total_questions']}ê°œ")
        print(f"  ì„±ê³µì  ë¶„ë¥˜: {summary['successful_classifications']}ê°œ")
        print(f"  ë¶„ë¥˜ ì„±ê³µë¥ : {summary['classification_success_rate']:.1f}%")
        print(f"  ëª…ë£Œí™” í•„ìš”: {summary['needs_clarification']}ê°œ")
        print(f"  ëª…ë£Œí™” ë¹„ìœ¨: {summary['clarification_rate']:.1f}%")
        
        print(f"\nğŸ¯ ì£¼ì œë³„ ë¶„í¬:")
        for topic, count in sorted(distributions['topics'].items()):
            percentage = count / summary['total_questions'] * 100
            print(f"  {topic}: {count}ê°œ ({percentage:.1f}%)")
        
        print(f"\nâ“ ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬:")
        for q_type, count in sorted(distributions['question_types'].items()):
            percentage = count / summary['total_questions'] * 100
            print(f"  {q_type}: {count}ê°œ ({percentage:.1f}%)")
        
        print(f"\nğŸ“š ë‚œì´ë„ë³„ ë¶„í¬:")
        for difficulty, count in sorted(distributions['difficulties'].items()):
            percentage = count / summary['total_questions'] * 100
            print(f"  {difficulty}: {count}ê°œ ({percentage:.1f}%)")
        
        print(f"\nğŸ‘¨â€ğŸ“ í•™ìƒ ìˆ˜ì¤€ë³„ ë¶„í¬:")
        for level, count in sorted(distributions['student_levels'].items()):
            percentage = count / summary['total_questions'] * 100
            print(f"  {level}: {count}ê°œ ({percentage:.1f}%)")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì§ˆë¬¸ë¶„ë¥˜ ë° ëª…ë£Œí™” ë‹¨ê³„ í…ŒìŠ¤í„° ì‹œì‘")
    print("="*60)
    
    # í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = QuestionClassificationTester()
    
    if not tester.openai_client:
        print("âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¡œë“œ
    test_file = 'output/deepfake_test_results_20250818_070138.json'
    if not tester.load_test_questions(test_file):
        print("âŒ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print(f"ğŸ“‹ {len(tester.test_questions)}ê°œ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    await tester.run_batch_test(batch_size=20, delay=2.0)
    
    # ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥
    tester.print_summary()
    
    # ê²°ê³¼ ì €ì¥
    output_file = tester.save_results()
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")

if __name__ == "__main__":
    asyncio.run(main())
